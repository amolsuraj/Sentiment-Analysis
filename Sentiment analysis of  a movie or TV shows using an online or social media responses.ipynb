{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2454dfba",
   "metadata": {},
   "source": [
    "# Problem statement – \n",
    "Create a data story of the online or social media response of a movie or a TV series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c88c7b",
   "metadata": {},
   "source": [
    "# Instructions:\n",
    "1. Select one recently launched movie or a TV show from Hotstar or Netlifx. \n",
    "2. Extract reviews, tweets, or any relevant text data from social media platforms or websites like Twitter, Facebook, Google etc.\n",
    "3. Clean the data and create an appropriate schema to store it in a table format(s).\n",
    "4. Perform EDA and apply relevant ML algorithms if required.\n",
    "5. Highlight insights or relevant stats and conclude whether the movie or TV series has received a positive, negative or neutral response from the online community.\n",
    "6. Record your outputs as a presentation or a dashboard.\n",
    "7. Share the following outputs\n",
    "    \n",
    "    a. PDF file of your presentation OR a dashboard link to an online public library (Example Tableau public or Power BI gallery).\n",
    "    \n",
    "    b. Supporting documents in PDF format – code, data, approach etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4387361c",
   "metadata": {},
   "source": [
    "## Mapping to ML problem:\n",
    "\n",
    "1. It is multiclass classification problem with 3 classes: Positive, Neutral and Negative.\n",
    "2. Data is scrapped from IMDB website using Selenium and BeutifulSoup which contains, review_title, review_text and review_rating in the range 1 to 10.\n",
    "**Performance matrix for Multiclass classification:**\n",
    "    \n",
    "    a. Multiclass log-loss (Cross entropy loss): Cross entropy loss is a metric used to measure how well a classification model in machine learning performs. The loss (or error) is measured as a number between 0 and 1, with 0 being a perfect model. The goal is generally to get your model as close to 0 as possible.Cross entropy loss measures the difference between the discovered probability distribution of a machine learning classification model and the predicted distribution. \n",
    "    b. Precision, Recall, f1_micro, Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec916bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\programdata\\anaconda3\\lib\\site-packages (4.1.5)\n",
      "Requirement already satisfied: urllib3[secure,socks]~=1.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (1.26.7)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (0.20.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.2)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.14.6)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: outcome in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.20)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.1.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pyOpenSSL>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (21.0.0)\n",
      "Requirement already satisfied: cryptography>=1.3.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (3.4.8)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
      "Requirement already satisfied: six>=1.5.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyOpenSSL>=0.14->urllib3[secure,socks]~=1.26->selenium) (1.16.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a889e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "# pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cb6b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #Using panda to create our dataframe\n",
    "# Import Selenium and its sub libraries\n",
    "import selenium \n",
    "from selenium import webdriver\n",
    "# Import BS4\n",
    "import requests #needed to load the page for BS4\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3ee1850",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r\"C:\\Users\\DS_USER\\Desktop\\Company Preparation Resume\\Data Sceince Resumes\\Resume supported Project Data\\4_Sentiment Analysis of Movie review\\chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "303eda28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://hungpham89.medium.com/a-beginner-guide-for-scraping-data-from-imdb-for-user-reviews-using-selenium-and-beautifulsoup-c60e89a4ad1a\n",
    "def get_review():\n",
    "    '''\n",
    "    Get the review from input as url for IMDB movies list.\n",
    "    The function takes 2 input the url of the movies and the name of the folder to store the data\n",
    "    For each folder, the function will grab the review for each movies and store into respective \n",
    "    file.\n",
    "    '''\n",
    "\n",
    "    #After the webpage opened, we can extract the title, hyperlink, year of each movies\n",
    "    #Set initial empty list for each element:\n",
    "\n",
    "    url = 'https://www.imdb.com/title/tt6468322/'\n",
    "    #setup user agent for BS4, except some rare case, it would be the same for most browser \n",
    "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "    #Use request.get to load the whole page\n",
    "    response = requests.get(url, headers = user_agent)\n",
    "    #Parse the request object to BS4 to transform it into html structure\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    #Find the link marked by the USER REVIEWS link text.\n",
    "    review_link = url+soup.find('a', text = 'User reviews').get('href')\n",
    "    #print(review_link)\n",
    "    driver = webdriver.Chrome(PATH)\n",
    "    driver.get(review_link)\n",
    "    driver.implicitly_wait(4) # tell the webdriver to wait for 1 seconds for the page to load to prevent blocked by anti spam software\n",
    "\n",
    "\n",
    "    # Set up action to click on 'load more' button\n",
    "    # note that each page on imdb has 25 reviews\n",
    "    page =1 #Set initial variable for while loop\n",
    "    #We want at least 1000 review, so get 50 at a safe number\n",
    "    while page<500:\n",
    "        try:\n",
    "            #find the load more button on the webpage\n",
    "            load_more = driver.find_element('load-more-trigger')\n",
    "            #click on that button\n",
    "            load_more.click()\n",
    "            page+=1 #move on to next loadmore button\n",
    "            \n",
    "        except:\n",
    "            #If couldnt find any button to click, stop\n",
    "            break\n",
    "    print(page)\n",
    "    # After fully expand the page, we will grab data from whole website\n",
    "    review = driver.find_elements_by_class_name('review-container')\n",
    "    #Set list for each element:\n",
    "    title = []\n",
    "    content = []\n",
    "    rating = []\n",
    "    date = []\n",
    "    user_name = []\n",
    "    #run for loop to get \n",
    "    for n in range(0,5000):\n",
    "        try:\n",
    "            #Some reviewers only give review text or rating without the other, \n",
    "            #so we use try/except here to make sure each block of content must has all the element before append them to the list\n",
    "\n",
    "            #Check if each review has all the elements\n",
    "            ftitle = review[n].find_element_by_class_name('title').text\n",
    "            #For the review content, some of them are hidden as spoiler, \n",
    "            #so we use the attribute 'textContent' here after extracting the 'content' tag\n",
    "            fcontent = review[n].find_element_by_class_name('content').get_attribute(\"textContent\").strip()\n",
    "            frating = review[n].find_element_by_class_name('rating-other-user-rating').text\n",
    "            fdate = review[n].find_element_by_class_name('review-date').text\n",
    "            fname = review[n].find_element_by_class_name('display-name-link').text\n",
    "\n",
    "\n",
    "            #Then add them to the respective list\n",
    "            title.append(ftitle)\n",
    "            content.append(fcontent)\n",
    "            rating.append(frating)\n",
    "            date.append(fdate)\n",
    "            user_name.append(fname)\n",
    "        except:\n",
    "            continue\n",
    "    #Build data dictionary for dataframe\n",
    "    data = {'User_name': user_name, \n",
    "            'Review title': title, \n",
    "            'Review Rating': rating,\n",
    "            'Review date' : date,\n",
    "            'Review_body' : content\n",
    "           }\n",
    "    #Build dataframe for each movie to export\n",
    "    print('Scraping Done')\n",
    "    review = pd.DataFrame(data = data)    \n",
    "    review['Movie_name'] = 'Money_heist' #create new column with the same movie name column    \n",
    "    review.to_csv('IMDB_scrapped.csv') #store them into individual file for each movies, so we can combine or check them later\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b2179e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DS_USER\\AppData\\Local\\Temp/ipykernel_23456/701739509.py:23: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(PATH)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DS_USER\\AppData\\Local\\Temp/ipykernel_23456/701739509.py:45: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  review = driver.find_elements_by_class_name('review-container')\n",
      "C:\\Users\\DS_USER\\AppData\\Local\\Temp/ipykernel_23456/701739509.py:59: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  ftitle = review[n].find_element_by_class_name('title').text\n",
      "C:\\Users\\DS_USER\\AppData\\Local\\Temp/ipykernel_23456/701739509.py:62: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  fcontent = review[n].find_element_by_class_name('content').get_attribute(\"textContent\").strip()\n",
      "C:\\Users\\DS_USER\\AppData\\Local\\Temp/ipykernel_23456/701739509.py:63: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  frating = review[n].find_element_by_class_name('rating-other-user-rating').text\n",
      "C:\\Users\\DS_USER\\AppData\\Local\\Temp/ipykernel_23456/701739509.py:64: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  fdate = review[n].find_element_by_class_name('review-date').text\n",
      "C:\\Users\\DS_USER\\AppData\\Local\\Temp/ipykernel_23456/701739509.py:65: DeprecationWarning: find_element_by_class_name is deprecated. Please use find_element(by=By.CLASS_NAME, value=name) instead\n",
      "  fname = review[n].find_element_by_class_name('display-name-link').text\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Done\n"
     ]
    }
   ],
   "source": [
    "get_review()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d121e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gensim is a Python library for topic modelling, document indexing and similarity retrieval \n",
    "# with large corpus. Target audience is the natural language processing (NLP) and information\n",
    "# retrieval (IR) community.\n",
    "# Gensim = “Generate Similar” is a popular open source natural language processing (NLP) \n",
    "# library used for unsupervised topic modeling.\n",
    "\n",
    "# pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b852b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# The Natural Language Toolkit (NLTK) is a platform used for building Python programs \n",
    "# that work with human language data for applying in statistical natural language processing (NLP). \n",
    "# It contains text processing libraries for tokenization, parsing, classification, stemming, \n",
    "# tagging and semantic reasoning.\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# TF-IDF is better than Count Vectorizers because it not only focuses on the frequency of words \n",
    "# present in the corpus but also provides the importance of the words. We can then remove the \n",
    "# words that are less important for analysis, hence making the model building less complex by \n",
    "# reducing the input dimensions.\n",
    "\n",
    "# TfidfTransformer: Transform a count matrix to a normalized tf or tf-idf representation.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# TfidfVectorizer: Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# CountVectorizer: Convert a collection of text documents to a matrix of token counts.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "# An ROC curve (receiver operating characteristic curve) is a graph showing the performance \n",
    "# of a classification model at all classification thresholds. \n",
    "from sklearn.metrics import roc_curve, auc\n",
    "# Stemmers remove morphological affixes from words, leaving only the word stem.\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re\n",
    "# Tutorial about Python regular expressions: https://pymotw.com/2/re/\n",
    "import string\n",
    "# In natural language processing, useless words (data), are referred to as stop words. \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#from gensim.models import Word2Vec\n",
    "#from gensim.models import KeyedVectors\n",
    "import pickle\n",
    "#tqdm is a Python library for adding progress bar. \n",
    "#It lets you configure and display a progress bar with metrics you want to track.\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff4317d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>User_name</th>\n",
       "      <th>Review title</th>\n",
       "      <th>Review Rating</th>\n",
       "      <th>Review date</th>\n",
       "      <th>Review_body</th>\n",
       "      <th>Movie_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>lee_eisenberg</td>\n",
       "      <td>theft of heft</td>\n",
       "      <td>10/10</td>\n",
       "      <td>24 August 2021</td>\n",
       "      <td>One of the many great series on Netflix depict...</td>\n",
       "      <td>Money_heist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ma-cortes</td>\n",
       "      <td>Awesome Spanish series with plenty of thrills ...</td>\n",
       "      <td>8/10</td>\n",
       "      <td>24 November 2018</td>\n",
       "      <td>Creator Alex Pina's last one results to be a s...</td>\n",
       "      <td>Money_heist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>searchanddestroy-1</td>\n",
       "      <td>What a mess!!!!</td>\n",
       "      <td>1/10</td>\n",
       "      <td>13 June 2018</td>\n",
       "      <td>I expected far better than this. This Tv serie...</td>\n",
       "      <td>Money_heist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>grantss</td>\n",
       "      <td>Clever and intriguing, initially, but gets dum...</td>\n",
       "      <td>5/10</td>\n",
       "      <td>9 January 2019</td>\n",
       "      <td>A band of robbers, lead by a man known simply ...</td>\n",
       "      <td>Money_heist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>deloudelouvain</td>\n",
       "      <td>Watch it in Spanish.</td>\n",
       "      <td>8/10</td>\n",
       "      <td>31 July 2020</td>\n",
       "      <td>All my friends were talking about La Casa De P...</td>\n",
       "      <td>Money_heist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0           User_name  \\\n",
       "0           0       lee_eisenberg   \n",
       "1           1           ma-cortes   \n",
       "2           2  searchanddestroy-1   \n",
       "3           3             grantss   \n",
       "4           4      deloudelouvain   \n",
       "\n",
       "                                        Review title Review Rating  \\\n",
       "0                                      theft of heft         10/10   \n",
       "1  Awesome Spanish series with plenty of thrills ...          8/10   \n",
       "2                                    What a mess!!!!          1/10   \n",
       "3  Clever and intriguing, initially, but gets dum...          5/10   \n",
       "4                               Watch it in Spanish.          8/10   \n",
       "\n",
       "        Review date                                        Review_body  \\\n",
       "0    24 August 2021  One of the many great series on Netflix depict...   \n",
       "1  24 November 2018  Creator Alex Pina's last one results to be a s...   \n",
       "2      13 June 2018  I expected far better than this. This Tv serie...   \n",
       "3    9 January 2019  A band of robbers, lead by a man known simply ...   \n",
       "4      31 July 2020  All my friends were talking about La Casa De P...   \n",
       "\n",
       "    Movie_name  \n",
       "0  Money_heist  \n",
       "1  Money_heist  \n",
       "2  Money_heist  \n",
       "3  Money_heist  \n",
       "4  Money_heist  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('IMDB_scrapped.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8e4717",
   "metadata": {},
   "source": [
    "## 1.1 Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2578c8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points in train data (3466, 7)\n",
      "--------------------------------------------------\n",
      "The attributes of data : ['Unnamed: 0' 'User_name' 'Review title' 'Review Rating' 'Review date'\n",
      " 'Review_body' 'Movie_name']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of data points in train data\", df.shape)\n",
    "print('-'*50)\n",
    "print(\"The attributes of data :\", df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "82be0a14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>User_name</th>\n",
       "      <th>Review title</th>\n",
       "      <th>Review Rating</th>\n",
       "      <th>Review date</th>\n",
       "      <th>Review_body</th>\n",
       "      <th>Movie_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>lee_eisenberg</td>\n",
       "      <td>theft of heft</td>\n",
       "      <td>10</td>\n",
       "      <td>24 August 2021</td>\n",
       "      <td>One of the many great series on Netflix depict...</td>\n",
       "      <td>Money_heist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ma-cortes</td>\n",
       "      <td>Awesome Spanish series with plenty of thrills ...</td>\n",
       "      <td>8</td>\n",
       "      <td>24 November 2018</td>\n",
       "      <td>Creator Alex Pina's last one results to be a s...</td>\n",
       "      <td>Money_heist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>searchanddestroy-1</td>\n",
       "      <td>What a mess!!!!</td>\n",
       "      <td>1</td>\n",
       "      <td>13 June 2018</td>\n",
       "      <td>I expected far better than this. This Tv serie...</td>\n",
       "      <td>Money_heist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>grantss</td>\n",
       "      <td>Clever and intriguing, initially, but gets dum...</td>\n",
       "      <td>5</td>\n",
       "      <td>9 January 2019</td>\n",
       "      <td>A band of robbers, lead by a man known simply ...</td>\n",
       "      <td>Money_heist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>deloudelouvain</td>\n",
       "      <td>Watch it in Spanish.</td>\n",
       "      <td>8</td>\n",
       "      <td>31 July 2020</td>\n",
       "      <td>All my friends were talking about La Casa De P...</td>\n",
       "      <td>Money_heist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3461</th>\n",
       "      <td>3461</td>\n",
       "      <td>mobilelunchbox</td>\n",
       "      <td>idea is good but messed up characters with man...</td>\n",
       "      <td>4</td>\n",
       "      <td>22 February 2019</td>\n",
       "      <td>The idea is good but messed up characters with...</td>\n",
       "      <td>Money_heist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>3462</td>\n",
       "      <td>DARK10x</td>\n",
       "      <td>Shame</td>\n",
       "      <td>1</td>\n",
       "      <td>5 September 2021</td>\n",
       "      <td>Some of the creators and actors supporting chi...</td>\n",
       "      <td>Money_heist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>3463</td>\n",
       "      <td>mudassiruabdullahi</td>\n",
       "      <td>The movie sweet die! (Another way of saying th...</td>\n",
       "      <td>10</td>\n",
       "      <td>5 April 2020</td>\n",
       "      <td>How i wish there is morethan 10stars for me to...</td>\n",
       "      <td>Money_heist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>3464</td>\n",
       "      <td>tanveersunny</td>\n",
       "      <td>Just awesome</td>\n",
       "      <td>10</td>\n",
       "      <td>5 September 2019</td>\n",
       "      <td>One of the best TV shows I've watched so far. ...</td>\n",
       "      <td>Money_heist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>3465</td>\n",
       "      <td>kfeshari</td>\n",
       "      <td>Difficult task for the writers</td>\n",
       "      <td>9</td>\n",
       "      <td>23 July 2019</td>\n",
       "      <td>After the ending of Part 2 of the series, I wa...</td>\n",
       "      <td>Money_heist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3466 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0           User_name  \\\n",
       "0              0       lee_eisenberg   \n",
       "1              1           ma-cortes   \n",
       "2              2  searchanddestroy-1   \n",
       "3              3             grantss   \n",
       "4              4      deloudelouvain   \n",
       "...          ...                 ...   \n",
       "3461        3461      mobilelunchbox   \n",
       "3462        3462             DARK10x   \n",
       "3463        3463  mudassiruabdullahi   \n",
       "3464        3464        tanveersunny   \n",
       "3465        3465            kfeshari   \n",
       "\n",
       "                                           Review title  Review Rating  \\\n",
       "0                                         theft of heft             10   \n",
       "1     Awesome Spanish series with plenty of thrills ...              8   \n",
       "2                                       What a mess!!!!              1   \n",
       "3     Clever and intriguing, initially, but gets dum...              5   \n",
       "4                                  Watch it in Spanish.              8   \n",
       "...                                                 ...            ...   \n",
       "3461  idea is good but messed up characters with man...              4   \n",
       "3462                                              Shame              1   \n",
       "3463  The movie sweet die! (Another way of saying th...             10   \n",
       "3464                                       Just awesome             10   \n",
       "3465                     Difficult task for the writers              9   \n",
       "\n",
       "           Review date                                        Review_body  \\\n",
       "0       24 August 2021  One of the many great series on Netflix depict...   \n",
       "1     24 November 2018  Creator Alex Pina's last one results to be a s...   \n",
       "2         13 June 2018  I expected far better than this. This Tv serie...   \n",
       "3       9 January 2019  A band of robbers, lead by a man known simply ...   \n",
       "4         31 July 2020  All my friends were talking about La Casa De P...   \n",
       "...                ...                                                ...   \n",
       "3461  22 February 2019  The idea is good but messed up characters with...   \n",
       "3462  5 September 2021  Some of the creators and actors supporting chi...   \n",
       "3463      5 April 2020  How i wish there is morethan 10stars for me to...   \n",
       "3464  5 September 2019  One of the best TV shows I've watched so far. ...   \n",
       "3465      23 July 2019  After the ending of Part 2 of the series, I wa...   \n",
       "\n",
       "       Movie_name  \n",
       "0     Money_heist  \n",
       "1     Money_heist  \n",
       "2     Money_heist  \n",
       "3     Money_heist  \n",
       "4     Money_heist  \n",
       "...           ...  \n",
       "3461  Money_heist  \n",
       "3462  Money_heist  \n",
       "3463  Money_heist  \n",
       "3464  Money_heist  \n",
       "3465  Money_heist  \n",
       "\n",
       "[3466 rows x 7 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in df['Review Rating']:\n",
    "    if type(i)== int:\n",
    "        continue\n",
    "    else:\n",
    "        temp=i.replace('/10','')\n",
    "        df['Review Rating'].replace({i:int(temp)},inplace=True)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9b561765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3466.000000\n",
       "mean        6.641373\n",
       "std         3.434366\n",
       "min         1.000000\n",
       "25%         3.000000\n",
       "50%         8.000000\n",
       "75%        10.000000\n",
       "max        10.000000\n",
       "Name: Review Rating, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Review Rating'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d23bcc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of Review Ratings: \n",
      " 2      157\n",
      "4      165\n",
      "3      184\n",
      "6      186\n",
      "5      198\n",
      "7      215\n",
      "8      247\n",
      "9      374\n",
      "1      540\n",
      "10    1200\n",
      "Name: Review Rating, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Distribution of Review Ratings: \\n\", df['Review Rating'].value_counts(ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b81414a",
   "metadata": {},
   "source": [
    "### Most of the user given ratings are 8, 9, 1 and 10. Top i.e. 10 ratings are given by 1200 users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01dddf0",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575e599e",
   "metadata": {},
   "source": [
    "## Using PDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "02b9426b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 296x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "g=sns.FacetGrid(df, size=4)\n",
    "g=g.map(sns.distplot, 'Review Rating').add_legend().set(title='*Feature = Review Ratings*')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff28d8",
   "metadata": {},
   "source": [
    "# Lets look at some text reviews to understand polarity of reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2cececbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I tuned into this hoping for a decent and clever bank heist tale. In some parts I got this.It feels like they first of all gave the script to some intelligent writers who conceived of these brilliant schemes to get into the bank, deal with the police and plan their escape. The kinds of antics that make the audience think 'wow, these guys are pretty smart'. The stakes are big, the planning is meticulous and the huge rewards worthy of the effort.When that was done, they then gave the script to a bunch of horny, adolescent teenagers who must have previously written 80's Australian soap operas. It is terrible. From establishing the actual job, it descends into a series of foolish characters who literally cannot keep their hands off each other and cannot keep their head in the game. They're shagging in vaults and toilets and offices. They're shagging when they are casing the joint, when they are doing the job and even with the lead investigator.From a job offering untold riches and the commitment of 5 weeks and the time for the heist, nobody can stay focused on what they are doing. They utter these silly speeches about how committed they are to the job and will kill anyone stopping it from happening but then in the next scene are willing to sacrifice it all for their sexual urges or the chance to be nice to hostages.So at the beginning, you are on the side of the criminals because their plan is so audacious but as it progresses you become more disinterested in their overly dramatic histrionics and complete lack of professionalism. There was one laughable scene where they were having another internal dispute and literally no one was supervising the hostages.Now, I'm sure people will say that you need to develop characters and yes you do but when those characters are acting so immaturely, so petulantly and so lacking in thought or consideration of the bigger picture, you just kinda get bored with them. You start looking for the most stupid and heist wrecking thing that they could do and laughing when they do it. I didn't tune in for a romantic drama but a bank heist and when the characters are behaving like silly teenagers, the tethers to your interest just withers away.You just want someone to shake these people and say 'we're talking about millions of Euro's each here. Is it possible you could remain on target for literally just the time of the heist (2/3 days) and then play out your love chasing routines when the job is done?' But even the guy at the very top is distracted by skirt so they all descend into silly unengaging children playing robbers.So overall, it was a fascinating concept that was ruined, and I do mean ruined, by characters acting so idiotically that they become laughable and unlikeable. They go from the no nonsense crew in Heat to a bunch of sexed up moronic clowns.\n"
     ]
    }
   ],
   "source": [
    "# Lets print one random review text to check its polarity.\n",
    "t=df[['Review_body']].loc[df['Review Rating']==3]\n",
    "print(t['Review_body'][67])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "471b262f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(881, 1)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Negative_rating= df[['Review_body']].loc[df['Review Rating']<4]\n",
    "Negative_rating.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0919cb1",
   "metadata": {},
   "source": [
    "### If we see the above review, we can easily guess that this review is totally negative. So the reviews with rating 3 and less will be considered as the negative review for this analysis task. There are 881 Negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9405c324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It has some good and unexpected moments, but also lots of thin plot points. It's written in the usual millennial style: lengthen the thing again and again. And again.But the worst aspect is watching the characters having relationships and often taking the dumbest possible decisions. I would like to say they are totally unrealistic, but it's worse, it's like watching the Big Brother on tv: a bunch of crazy, narcissistic and brainless guys, fu**ing, fighting and switching sides all the time.Tokyo, Rio and Angel are extremely unlikeable, Nairobi and Denver almost are. Some of them are very badly acted. On the other hand, Berlin and Moscow are interesting and well acted characters. The Professor is just, well, the Professor.\n",
      "                \n",
      "                    42 out of 67 found this helpful.\n",
      "                        \n",
      "                            Was this review helpful?  Sign in to vote.\n",
      "                        \n",
      "                        \n",
      "                    Permalink\n"
     ]
    }
   ],
   "source": [
    "t=df[['Review_body']].loc[df['Review Rating']==4]\n",
    "print(t['Review_body'][44])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c8769356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I give 6.5 but rounded up. First off you need to get past the ridiculous plot that seeks to garner sympathy for criminals. Has the obligatory Netflix PC sub-plots and a few very annoying characters. But still very well produced and even enjoyable despite gaping holes in the story and absolutely unbelievable action sequences. Guns being pulled and pointed at each other constantly is definitely eye rolling material. Acting is above average and dialogue rings true. Some real depth of emotion in the characters emerge. Seasons 1& 2 are far superior to 3 & 4.\n",
      "                \n",
      "                    1 out of 5 found this helpful.\n",
      "                        \n",
      "                            Was this review helpful?  Sign in to vote.\n",
      "                        \n",
      "                        \n",
      "                    Permalink\n"
     ]
    }
   ],
   "source": [
    "t=df[['Review_body']].loc[df['Review Rating']==7]\n",
    "print(t['Review_body'][46])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "aa81b297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764, 1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Neutral_rating= df[['Review_body']].loc[((df['Review Rating']>=4)&(df['Review Rating']<8))]\n",
    "Neutral_rating.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e484bf",
   "metadata": {},
   "source": [
    "### If we see the above reviews, There are some positive as well as negative aspects are mentioned. So the reviews with rating between 4 and 7 will be considered as the Neutral review for this analysis task. There are 764 Neutral reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "235f0d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leaving out the fact that this, is one of the greatest things Spain ever made (after some horror films), I must say that I wasn't fully satisfied about some logics during the episodes. \"El Professor\" obviously was the most intelligent character of the series, but some dynamics they were left to chance. Fortunatly, everything ended up to his advantage, but I hoped that nothing was by chance, like I said. Apart from that, the final left me a little bit down, like many others because during the episodes they made me fond of the characters for leaving me with too many questions. Anyway, I really enjoyed it and hope it's not the last season!\n",
      "                \n",
      "                    4 out of 8 found this helpful.\n",
      "                        \n",
      "                            Was this review helpful?  Sign in to vote.\n",
      "                        \n",
      "                        \n",
      "                    Permalink\n"
     ]
    }
   ],
   "source": [
    "t=df[['Review_body']].loc[df['Review Rating']==8]\n",
    "print(t['Review_body'][3315])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "25efdeb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1821, 1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Positive_rating= df[['Review_body']].loc[df['Review Rating']>=8]\n",
    "Positive_rating.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61741d13",
   "metadata": {},
   "source": [
    "### If we see the above review, we can easily guess that this review is totally Positive. So the reviews with rating 8 and above will be considered as the Positive review for this analysis task. There are 1821 Positive reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a39963",
   "metadata": {},
   "source": [
    "# Preprocessing of text data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16caa308",
   "metadata": {},
   "source": [
    "1) If we see Review_title also has some information about the sentiments of the user. Also it describes whole review in few words. Hence we will use this feature also.\n",
    "\n",
    "2) For this analysis we are going to use three features from datframe which are Review_title, Review_body and Review_rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55461794",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4588/1723198512.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Review title'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Review_body'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Review Rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "data= df[['Review title','Review_body','Review Rating']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ea74e571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                           theft of heft\n",
       "1       Awesome Spanish series with plenty of thrills ...\n",
       "2                                         What a mess!!!!\n",
       "3       Clever and intriguing, initially, but gets dum...\n",
       "4                                    Watch it in Spanish.\n",
       "                              ...                        \n",
       "3461    idea is good but messed up characters with man...\n",
       "3462                                                Shame\n",
       "3463    The movie sweet die! (Another way of saying th...\n",
       "3464                                         Just awesome\n",
       "3465                       Difficult task for the writers\n",
       "Name: Review title, Length: 3466, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Review title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eeeb0f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some special characters, stop words, brackets in Review title texts.\n",
    "#Lets preprocess it\n",
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "# Contractions are words or combinations of words that are shortened by dropping letters \n",
    "# and replacing them by an apostrophe. Contractions means shorthand words. \n",
    "# Decontracted means make then long-hand words.\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# https://gist.github.com/sebleier/554280\n",
    "# we are removing the words from the stop words list: 'no', 'nor', 'not' as they \n",
    "# contribute in Negative reviews\n",
    "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "\n",
    " \n",
    "from tqdm import tqdm\n",
    "def preprocess_text(text_data):\n",
    "    preprocessed_text = []\n",
    "    # tqdm is for printing the status bar\n",
    "    for sentance in tqdm(text_data):\n",
    "        sent = decontracted(sentance)\n",
    "        sent = sent.replace('\\\\r', ' ')\n",
    "        sent = sent.replace('\\\\n', ' ')\n",
    "        sent = sent.replace('\\\\\"', ' ')\n",
    "        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "        sent = re.sub(r\"<.*>\",\"\",sent, flags=re.MULTILINE)\n",
    "        sent = re.sub(r\"[\\n\\t\\-\\\\\\/]\",\" \",sent, flags=re.MULTILINE)\n",
    "        sent = re.sub(r\"\\b_([a-zA-z]+)_\\b\",r\"\\1\",sent)  # to replace _word_ to word\n",
    "        sent = re.sub(r\"\\b_([a-zA-z]+)\\b\",r\"\\1\",sent)   # to replace_word to word\n",
    "        sent = re.sub(r\"\\b([a-zA-z]+)_\\b\",r\"\\1\",sent)   # to replace word_ to word\n",
    "        sent = re.sub(r'\\b\\w{1,2}\\b',\" \",sent) #remove words <2\n",
    "        sent = re.sub(r\"\\b\\w{15,}\\b\",\" \",sent) #remove words >15\n",
    "        sent = re.sub(r'\\d',\"\",sent, flags=re.MULTILINE)\n",
    "        \n",
    "        # https://gist.github.com/sebleier/554280\n",
    "        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n",
    "        preprocessed_text.append(sent.lower().strip())\n",
    "    return preprocessed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201df43a",
   "metadata": {},
   "source": [
    "### Preprocessing: Review title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "41cac88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 3466/3466 [00:00<00:00, 14389.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['theft heft',\n",
       " 'awesome spanish series plenty thrills action twists turns',\n",
       " 'mess',\n",
       " 'clever intriguing initially gets dumber dumber longer goes',\n",
       " 'watch spanish',\n",
       " 'casa papel',\n",
       " 'good enough',\n",
       " 'not forget breathe',\n",
       " 'unique unique',\n",
       " 'life',\n",
       " 'lady detective talks mummy going gets tough',\n",
       " 'slickly sexy spanish steal',\n",
       " 'season',\n",
       " 'one best series ever seen',\n",
       " 'every season gets even better',\n",
       " 'not used',\n",
       " 'raquel worst cop ever',\n",
       " 'great',\n",
       " 'quite intersting',\n",
       " 'wonderful show',\n",
       " 'end',\n",
       " 'absorbing paper series',\n",
       " 'parts bad parts good',\n",
       " 'masterpiece',\n",
       " 'intriguing different',\n",
       " 'clunker capital',\n",
       " 'would like say loved heist show tremendously went tad long',\n",
       " 'really bad soap opera worse school production not even laughable',\n",
       " 'awaiting final',\n",
       " 'honestly',\n",
       " 'superb',\n",
       " 'casa papel',\n",
       " 'every little thing drama drawn expanded',\n",
       " 'good overrated',\n",
       " 'started pretty good',\n",
       " 'pretty entertaining',\n",
       " 'awesome start gets somewhat boring toward season',\n",
       " 'money',\n",
       " 'ridiculous',\n",
       " 'show turned parody season review',\n",
       " 'not supposed continue',\n",
       " 'hysterical overacting',\n",
       " 'love hearing spanish',\n",
       " 'love show hate one thing',\n",
       " 'fatally flawed',\n",
       " 'starts great stop watching season thank',\n",
       " 'definitely overrated enjoyable',\n",
       " 'seasons cked',\n",
       " 'disappointing',\n",
       " 'missing',\n",
       " 'rarely watched something stupid',\n",
       " 'era defining global phenomenon',\n",
       " 'lmao',\n",
       " 'opinion',\n",
       " 'long series worth',\n",
       " 'basically season prison break intesnity spanish',\n",
       " 'loved seasons',\n",
       " 'hard stop watching',\n",
       " 'dream spanish culture',\n",
       " 'great start season terrible',\n",
       " 'fact',\n",
       " 'dios estoy boquiabierta que pedazo obra maestra',\n",
       " 'really wish ended season',\n",
       " 'great show',\n",
       " 'great show',\n",
       " 'one best heist series',\n",
       " 'good',\n",
       " 'clever ultimately childishly idiotic',\n",
       " 'not best show since breaking bad',\n",
       " 'gave chance',\n",
       " 'boring',\n",
       " 'perfection name reckon',\n",
       " 'not best show watched',\n",
       " 'casa papel',\n",
       " 'fuzz',\n",
       " 'overrated nonsense',\n",
       " 'review part',\n",
       " 'antagonist runs show',\n",
       " 'season',\n",
       " 'made seasons casa papel would solid',\n",
       " 'poo',\n",
       " 'one best series watched till date',\n",
       " 'season review good',\n",
       " 'emotional rollercoaster ride not expecting',\n",
       " 'well many mistakes sometime slow',\n",
       " 'long',\n",
       " 'another series netflix destroy',\n",
       " 'show unpredictable',\n",
       " 'tokyo perfect stupid woman',\n",
       " 'lost version heist',\n",
       " 'bella ciao welcome festival',\n",
       " 'money heist joie vivre',\n",
       " 'great series',\n",
       " 'average',\n",
       " 'good years old',\n",
       " 'could one time greatest',\n",
       " 'okay not good',\n",
       " 'lame writing',\n",
       " 'awesome drama',\n",
       " 'quit episode',\n",
       " 'first two season good terrible',\n",
       " 'great show showing black lives matter system',\n",
       " 'opinion',\n",
       " 'entertaining escapism',\n",
       " 'great premise inept delivery',\n",
       " 'light seven overrated unrealistic series',\n",
       " 'good action drama super naive silly scenarios',\n",
       " 'ridiculously overrated',\n",
       " 'absolutely gripping entertaining despite flaws conveniences',\n",
       " 'good hiest series',\n",
       " 'breaking bad meets ocean',\n",
       " 'bored tears',\n",
       " 'chewing gum',\n",
       " 'excess romance irritating couples',\n",
       " 'omg late party show remarkable',\n",
       " 'not live billing',\n",
       " 'classic thriller oomph dud moments',\n",
       " 'bad dumb bad really bad',\n",
       " 'surprisingly fantastic',\n",
       " 'much soap opera',\n",
       " 'stopped season',\n",
       " 'thrilling season',\n",
       " 'wow wow wow',\n",
       " 'season awesome not best',\n",
       " 'ejjjj',\n",
       " 'starting well becomes predictable repetitive',\n",
       " 'masterpiece',\n",
       " 'fair review among many extremes',\n",
       " 'unexpected ending',\n",
       " 'pleasure ending story discovering end',\n",
       " '',\n",
       " 'possible pull ultimate victimless crime',\n",
       " 'clever inventive bright',\n",
       " 'season blahh',\n",
       " 'overrated',\n",
       " 'intelligent thriller',\n",
       " 'buckle wild ride',\n",
       " 'polished frustratingly confused pacing',\n",
       " 'overrated',\n",
       " 'nice',\n",
       " 'incredibly entertaining',\n",
       " 'umm episode list season',\n",
       " 'incredible',\n",
       " 'binge watched something crazy',\n",
       " 'goes way long',\n",
       " 'nothing better watch',\n",
       " 'perhaps',\n",
       " 'best',\n",
       " 'started strong',\n",
       " 'not waste time grosss',\n",
       " 'art heist twist turn',\n",
       " 'lots nonsense afterwards even started good',\n",
       " 'good show',\n",
       " 'overhyped trash',\n",
       " 'ironic robbery professor',\n",
       " 'average show not great',\n",
       " 'perfect series binge watching',\n",
       " 'worth look spoilt absurd timeline',\n",
       " 'beware show past season',\n",
       " 'money heist',\n",
       " 'overacting horrible voice dubs',\n",
       " 'inside man bite',\n",
       " 'extremely soapy enjoyable logic importance',\n",
       " 'smart worth time',\n",
       " 'dissapointing',\n",
       " 'not supposed seasons',\n",
       " 'movie premyy honest review',\n",
       " 'season disappointment',\n",
       " 'one best movie ever seen',\n",
       " 'overrated',\n",
       " 'love heist',\n",
       " 'collapsed story writing',\n",
       " '',\n",
       " 'fast furious saga series style',\n",
       " 'many plot holes',\n",
       " 'lunacy not top',\n",
       " 'not perfect worth watching',\n",
       " 'brilliant edge',\n",
       " 'naive interesting',\n",
       " 'best show made netflix',\n",
       " 'dumb stupid',\n",
       " 'disappointed rd season basically people shouting top volume music',\n",
       " 'great season show',\n",
       " 'best series ever watched time',\n",
       " 'great writing performances',\n",
       " 'soap opera crime drama',\n",
       " 'dumb dumber',\n",
       " 'season',\n",
       " 'annoying soup opera drama',\n",
       " 'best thing netflix far',\n",
       " 'kill popular characters show',\n",
       " 'copycat',\n",
       " 'seen twice already',\n",
       " 'first seasons amazing',\n",
       " 'grand spectacular',\n",
       " 'series watch',\n",
       " 'nice season',\n",
       " 'overrated',\n",
       " 'thanks netflix',\n",
       " 'fantastic',\n",
       " 'heart pumping enjoyable',\n",
       " 'avoid',\n",
       " 'really facepalm',\n",
       " 'think ocean comprehensive',\n",
       " 'masterpice',\n",
       " 'pathetic waste time',\n",
       " 'god sake stop nonsense end show season',\n",
       " 'great content netflix',\n",
       " 'top heist movies ever',\n",
       " 'epic',\n",
       " 'season brilliant',\n",
       " 'one best shows netflix right',\n",
       " 'kill favs much',\n",
       " 'pace slow plot divided three routes',\n",
       " 'loved great movie',\n",
       " 'waste time',\n",
       " 'ocean eleven twenty chapters',\n",
       " 'season entertaining stupidity',\n",
       " 'overwhelmingly frustrating',\n",
       " 'boring day time soap opera terrible acting horrible voiceovers ugh',\n",
       " 'season outstanding never forget nairobi',\n",
       " 'papel',\n",
       " 'first second part',\n",
       " 'love season',\n",
       " 'ended season',\n",
       " 'gripping',\n",
       " 'crime soap opera',\n",
       " 'wonderful boring',\n",
       " 'bella ciao',\n",
       " 'hype coming',\n",
       " 'best serie ever',\n",
       " 'horribly done',\n",
       " 'brilliant thriller',\n",
       " 'thrillingly good',\n",
       " 'end ruin',\n",
       " 'not get hype',\n",
       " 'good',\n",
       " 'money heist must watch',\n",
       " 'till first seasons money heist epic',\n",
       " 'bravo well done',\n",
       " 'war started',\n",
       " 'long good',\n",
       " 'like one flashback another',\n",
       " 'one entertaining series ever',\n",
       " 'best netflix',\n",
       " 'speechless',\n",
       " 'enjoyed season',\n",
       " 'top quality',\n",
       " 'masterpiece',\n",
       " 'hyped',\n",
       " 'not flawless better netflix garbage',\n",
       " 'good way long',\n",
       " 'ultimate strategies heist',\n",
       " 'much longer',\n",
       " 'surprise',\n",
       " 'amateurish cringy',\n",
       " 'watch first seasons maybe',\n",
       " 'garbage heist',\n",
       " 'bad worse',\n",
       " 'finally back',\n",
       " 'twist turns mind games',\n",
       " 'season',\n",
       " 'megastars born',\n",
       " 'thrilling exciting better prison break',\n",
       " 'money heist',\n",
       " 'bad',\n",
       " 'downhill',\n",
       " 'terribly downhill',\n",
       " 'brilliant unique',\n",
       " 'episodes really',\n",
       " 'every season gettin worst',\n",
       " 'money heist back',\n",
       " 'game possibilities',\n",
       " 'pim pam pum',\n",
       " 'clear everyone soap opera',\n",
       " 'awesome',\n",
       " 'zany intelligent',\n",
       " 'good job netflix',\n",
       " 'best heist series',\n",
       " 'fan would say marvelous',\n",
       " 'watch original language version dubbed awful',\n",
       " 'decent',\n",
       " 'childish mistakes',\n",
       " 'ridiculous rubbish',\n",
       " 'beautiful series much character depth',\n",
       " 'overdone',\n",
       " 'difficult redo season',\n",
       " 'overrated',\n",
       " 'loved',\n",
       " 'entertainment highest level',\n",
       " 'casa papel',\n",
       " 'getting worse',\n",
       " '',\n",
       " 'downhill season',\n",
       " 'woooow',\n",
       " 'going downhill',\n",
       " 'absolutely incredible',\n",
       " 'mind boggling',\n",
       " 'one word',\n",
       " 'started good meh',\n",
       " 'good',\n",
       " 'epic',\n",
       " 'casa papel',\n",
       " 'terrible',\n",
       " 'masterpiece not understand hate not watch dubbed',\n",
       " 'season amazing rest',\n",
       " 'long',\n",
       " 'mediocre best way overrated',\n",
       " 'pretty bad',\n",
       " 'thumbs',\n",
       " 'using corpse first seasons',\n",
       " 'favorite spanish ever',\n",
       " 'way much plot',\n",
       " 'overrated',\n",
       " 'spaghetti western',\n",
       " 'great show shame american dubbing',\n",
       " 'lockdown binge watching',\n",
       " 'intelligence new sexy',\n",
       " 'casa papel',\n",
       " 'show fans need mental medication',\n",
       " 'seamless',\n",
       " 'star th season',\n",
       " 'nearly best heist plot',\n",
       " 'episodes kill',\n",
       " 'insanely overrated soap opera',\n",
       " 'realy good bid',\n",
       " 'ridiculous piece crap',\n",
       " 'casa papel',\n",
       " 'boring',\n",
       " 'masterpiece',\n",
       " 'great story',\n",
       " 'stop hyping show',\n",
       " 'not get hype',\n",
       " 'bad',\n",
       " 'least professional master criminals ever',\n",
       " 'real treat',\n",
       " 'absolutely baffled highly rated',\n",
       " 'cheesy',\n",
       " 'style substance brains',\n",
       " 'great probably one best netflix series seen far',\n",
       " 'much overrated',\n",
       " 'promising start',\n",
       " 'entertaining not well done',\n",
       " 'oke much boring talks',\n",
       " 'not perfect entertaining',\n",
       " 'cliche predictable',\n",
       " 'magnificent',\n",
       " 'looks good',\n",
       " 'rating not justfied',\n",
       " 'stop watching',\n",
       " 'medium passing time',\n",
       " 'season emotional roller coaster',\n",
       " 'awesome series heist',\n",
       " 'excellent',\n",
       " 'everyone critic',\n",
       " 'amo professor joder',\n",
       " 'drama',\n",
       " 'best show',\n",
       " 'sad example sequels driven greed destroy great show',\n",
       " 'good performances totally overhyped unrealistic crap show',\n",
       " 'not judge simply watch',\n",
       " 'smartest suspens story morden times',\n",
       " 'season last season',\n",
       " 'part like watching grade movie',\n",
       " 'awesome',\n",
       " 'masterpiece storytelling audio visual entertainment',\n",
       " 'amazing',\n",
       " 'childish drivel',\n",
       " 'really liked',\n",
       " 'casa papel',\n",
       " 'casa papel',\n",
       " 'rise fall',\n",
       " 'best series ever',\n",
       " 'nothing sadder watching favorite unique show turning absolute mediocre',\n",
       " 'pathetic series',\n",
       " 'dumb show',\n",
       " 'great garbage fantasy',\n",
       " 'boring show time',\n",
       " 'pretty illogical',\n",
       " 'horrible',\n",
       " 'rotted quickly',\n",
       " 'first seasons',\n",
       " 'took next level season',\n",
       " 'dont believe hype childish utterly overrated show',\n",
       " 'stoppen nd season',\n",
       " 'never rd season',\n",
       " 'honestly',\n",
       " 'masterpiece',\n",
       " 'season review',\n",
       " 'sometimes better stop',\n",
       " 'bella ciao',\n",
       " 'not give zero stars',\n",
       " 'casa papel',\n",
       " 'intriguing premise',\n",
       " 'going well til lazy writing swiss cheese plot purgatory',\n",
       " 'warning spoilers',\n",
       " 'hope netflix not come th season',\n",
       " 'time hit like soprano friends',\n",
       " 'offensive unwatchable',\n",
       " 'season second half expectations',\n",
       " 'frustrating',\n",
       " 'literally dumbest show',\n",
       " 'whole lot hype whole lot nothing',\n",
       " 'first season second good',\n",
       " 'entertaining series',\n",
       " 'destroy show',\n",
       " 'utterly bad illogical',\n",
       " 'netflix took',\n",
       " 'good characters always messing',\n",
       " '',\n",
       " 'tokyo ruined',\n",
       " 'casa papel',\n",
       " 'rambo professor midwife',\n",
       " 'not get sucked lol',\n",
       " 'probably echoing anything everything positive',\n",
       " 'money illogical heist',\n",
       " 'ver solo primer temporada',\n",
       " 'could stopped season',\n",
       " 'season',\n",
       " 'clich series not make mind',\n",
       " 'por favor deje please stop',\n",
       " 'never thought season would good',\n",
       " 'best shows along breaking bad',\n",
       " 'waited month shiiiiit ohh wawoo',\n",
       " 'really enjoyed season',\n",
       " 'great series',\n",
       " 'good plot wasted',\n",
       " 'one best heist series',\n",
       " 'great guilty pleasure binge',\n",
       " 'mixed bag',\n",
       " 'best show right breaking bad',\n",
       " 'changed rating cuz killed tokyo',\n",
       " 'best show indian audience',\n",
       " 'greatest crime century',\n",
       " 'excellent idea hands low grade students',\n",
       " 'dissapointing',\n",
       " 'disaster',\n",
       " 'plot crafting like',\n",
       " 'extraordinary',\n",
       " 'stayed casa papel',\n",
       " 'much good thing makes bad thing',\n",
       " 'best series ever',\n",
       " 'best series ever',\n",
       " 'seems like waste time',\n",
       " 'bella ciao',\n",
       " 'poor dubbing',\n",
       " 'watch first seasons',\n",
       " 'dubbing awful',\n",
       " 'almost amazing',\n",
       " 'best series watched',\n",
       " 'disappointing',\n",
       " 'mostar not bosnia mostar herzegovina',\n",
       " 'come come nobody bank recognises mob nobody recognised tokyo ended midway season',\n",
       " 'work unbelievable fiction pedro alonso berlin carried show',\n",
       " 'words',\n",
       " 'nice losing',\n",
       " 'casa papel',\n",
       " 'amazing',\n",
       " 'overated',\n",
       " 'not worth high ratings entertaining',\n",
       " 'boy really',\n",
       " 'thank brilliant ending',\n",
       " 'not getting better',\n",
       " 'marvellous issues beyond limit dirty language hindi human nature always talks sex',\n",
       " 'highly addictive',\n",
       " '',\n",
       " 'truly awful storytelling',\n",
       " 'wow',\n",
       " 'best show ever seen much love team',\n",
       " 'love magic realism love',\n",
       " 'yuck',\n",
       " 'absolutely brilliant',\n",
       " 'messy heist',\n",
       " 'seriously',\n",
       " 'shame',\n",
       " 'one really dies show',\n",
       " 'great',\n",
       " 'perfection',\n",
       " 'good',\n",
       " 'incredible first season falls apart towards end season',\n",
       " 'great series unfortunately dubbing rd rate',\n",
       " 'best spanish series',\n",
       " 'casa papel',\n",
       " 'relaxing one',\n",
       " 'amazing spanish drama',\n",
       " 'good time pass',\n",
       " 'excellent',\n",
       " 'work genious',\n",
       " 'not stop binge watching',\n",
       " 'dont watch english dubbing',\n",
       " 'masterpiece action flawed characters',\n",
       " 'wow edge seat chess game',\n",
       " 'ruin fourth season',\n",
       " 'omg forth season awful',\n",
       " 'overrated pathetic show mordern',\n",
       " 'real dumb robbers',\n",
       " 'deus machina',\n",
       " 'remembered greatest heist show mind blowing',\n",
       " 'great crime story would better without ideology',\n",
       " 'first two seasons really good becomes clear ran inspiration',\n",
       " 'watch stop',\n",
       " 'keeps interested',\n",
       " 'best web series watched',\n",
       " 'bad',\n",
       " 'farewell thank netflix thank professor team series decade',\n",
       " 'amazing',\n",
       " 'hats',\n",
       " 'nothing special',\n",
       " 'best crime thriller web series world',\n",
       " 'intelligent show',\n",
       " 'best series lifetime',\n",
       " 'first two seasons awesome',\n",
       " 'interesting begin boring season',\n",
       " 'extremely overrated',\n",
       " 'casa papel',\n",
       " 'brilliant show',\n",
       " 'great overhyped overrated',\n",
       " 'good part became mess part',\n",
       " 'interesting set',\n",
       " 'great story',\n",
       " 'overhyped average',\n",
       " 'makes series great',\n",
       " 'simply thank',\n",
       " 'overhyped',\n",
       " 'every second counts',\n",
       " 'awesome till netflix took',\n",
       " 'first seasons best th th really worst',\n",
       " 'wow one must see',\n",
       " 'typical spanish drama',\n",
       " 'blockbuster',\n",
       " 'trash',\n",
       " 'anything possible',\n",
       " 'season rating fell',\n",
       " 'great outrageous sexist dialogue',\n",
       " 'must watch',\n",
       " 'better not dubbed',\n",
       " 'definetely beyon',\n",
       " 'best',\n",
       " 'evolves comedy show point',\n",
       " 'telenovela type heist',\n",
       " 'one best shows watched',\n",
       " 'perfect masala entertainer',\n",
       " 'honestly liked',\n",
       " 'fantastic',\n",
       " 'superb',\n",
       " 'amazing',\n",
       " 'unusual series must see',\n",
       " 'great ending',\n",
       " 'one best',\n",
       " 'spoilers part absolute rubbish',\n",
       " 'not bad finishing seasons would make legend',\n",
       " 'second season bad',\n",
       " 'overrated',\n",
       " 'dont fooled ratings',\n",
       " 'great foreign show',\n",
       " 'goat',\n",
       " 'amazing',\n",
       " 'show not know wants',\n",
       " 'casa perceccion',\n",
       " 'meandering nonsensical plot bunch stereotypes',\n",
       " 'overrated pile ever',\n",
       " '',\n",
       " 'amazingly good show',\n",
       " 'disappointment',\n",
       " 'stopped afer season terrible crap',\n",
       " 'season hype train train wreck',\n",
       " 'incredible',\n",
       " 'not good',\n",
       " 'worth time good series ever netflix',\n",
       " 'rated hyped nonsense',\n",
       " 'good plot progressive holes',\n",
       " 'brilliant idea turning spanish telenovela',\n",
       " 'not splendid story executed superb craftmanship',\n",
       " 'spent seasons getting rid',\n",
       " 'shame',\n",
       " 'stars',\n",
       " 'another spanish soap opera time combined exciting bank robery',\n",
       " 'perfecto plan',\n",
       " 'laughable',\n",
       " 'absolutely awesome',\n",
       " 'well made show goes nowhere',\n",
       " 'not stop',\n",
       " 'good third season unwatchable',\n",
       " 'great tons unnecessary scenes',\n",
       " 'rated',\n",
       " 'overrated',\n",
       " 'lovely',\n",
       " 'heist rated not watch waste time',\n",
       " 'awsome',\n",
       " 'rated nonsense soap opera',\n",
       " 'slick gritty funny entertaining addictive plot twisting silly serious madness',\n",
       " 'not disappointed',\n",
       " 'boring core please not waste time',\n",
       " 'masterpiece kindly dont negative reviews',\n",
       " 'would recommend kids completely unrealistic',\n",
       " 'casa papel',\n",
       " 'one best series',\n",
       " 'overrated',\n",
       " 'casa paper money heist',\n",
       " 'really good nd heist meh',\n",
       " 'great show',\n",
       " 'pretty good',\n",
       " 'proof spanish also bad american',\n",
       " 'totally stupid plot',\n",
       " 'let fall love',\n",
       " 'could much',\n",
       " 'season conclusion best show ever',\n",
       " 'happens run ideas',\n",
       " 'overhyped',\n",
       " 'lead detective character ruined show',\n",
       " 'best series yet netflix far',\n",
       " 'entertaining poor',\n",
       " 'outstanding',\n",
       " 'god awful season',\n",
       " 'copy inside man denzel washintong clive owen',\n",
       " 'bollywood show',\n",
       " 'season bitter disappointment',\n",
       " 'missing something',\n",
       " 'far fetched bad acting',\n",
       " 'casting became joke',\n",
       " 'food long',\n",
       " 'disappointment',\n",
       " 'overhyped stupid series waste time',\n",
       " 'mind blowing game changing',\n",
       " 'thriller outstanding',\n",
       " 'amazing show watch spanish',\n",
       " 'story good makes sense',\n",
       " 'story good makes sense',\n",
       " 'actually good first seasons',\n",
       " 'watch give star',\n",
       " 'skyscraper ambition',\n",
       " 'cheesy irrational fake',\n",
       " 'greatest thing ever happen spanish',\n",
       " 'high rating',\n",
       " 'best ever',\n",
       " 'money heist noob watcher dream series',\n",
       " 'part',\n",
       " 'complete sht show',\n",
       " 'excellent small word moneyheist series',\n",
       " 'got really dull really quickly',\n",
       " 'brilliant concept heist much love drama spoil flow',\n",
       " 'season good started screwing',\n",
       " 'best international series',\n",
       " 'brilliant show',\n",
       " 'give extra point make',\n",
       " 'exciting',\n",
       " 'stunning fabulous brilliant else',\n",
       " '',\n",
       " 'amazing',\n",
       " 'telenov starts potential',\n",
       " 'best series',\n",
       " 'amazing series',\n",
       " 'masterpiece',\n",
       " 'not boring',\n",
       " 'worst part making season',\n",
       " 'stop hate please',\n",
       " 'captivating thrilling else need',\n",
       " 'best',\n",
       " 'optimistic thought',\n",
       " 'show overstays welcome',\n",
       " 'great show',\n",
       " 'tokyo first female may convince let cut hair',\n",
       " 'masterpiece series',\n",
       " 'stupid',\n",
       " 'narrative failure',\n",
       " 'professor childhood',\n",
       " 'addictive spectacular',\n",
       " 'think good guess not watch lot heist shows movies',\n",
       " 'getting better better',\n",
       " 'master plan',\n",
       " 'brilliant script',\n",
       " 'enjoyable annoying time',\n",
       " 'really cool exciting show',\n",
       " 'best web series ever seen',\n",
       " 'simply amazing',\n",
       " 'overrated',\n",
       " 'excellent series overall',\n",
       " 'lifetime series loved characters',\n",
       " 'amazing',\n",
       " 'got boring',\n",
       " 'put together well',\n",
       " 'entertaining',\n",
       " 'wooooooowwwww',\n",
       " 'stupid',\n",
       " 'kind crazy',\n",
       " 'ridiculous comical heist bloated exhausting stretched drama',\n",
       " 'brilliant still',\n",
       " 'season great not think whole show garbage',\n",
       " 'maybe best series ever seen',\n",
       " '',\n",
       " 'gets worse worse',\n",
       " 'pitch perfect',\n",
       " 'sucks',\n",
       " 'completely ruined',\n",
       " 'stopped season',\n",
       " 'fantastic excellent must watch',\n",
       " 'phenomenom',\n",
       " 'not okay',\n",
       " 'outstanding',\n",
       " 'dissapointing endending ever season',\n",
       " 'need ending',\n",
       " 'must see',\n",
       " 'well least good spanish',\n",
       " 'season masterpiece',\n",
       " 'saw could find reviews called casa papel house paper unbiased review',\n",
       " 'bad wasting time',\n",
       " 'european',\n",
       " 'incredible passionate intricate funny heist',\n",
       " 'incredible',\n",
       " 'great show',\n",
       " 'ridiculous',\n",
       " 'english dubbing poor',\n",
       " 'much love heist complete rubbish',\n",
       " 'god bad annoying',\n",
       " 'nobody dies',\n",
       " 'show problematic',\n",
       " 'let cut crap honest',\n",
       " 'captivating',\n",
       " 'bingy worthy',\n",
       " 'acting storyline',\n",
       " 'fine',\n",
       " 'waiting next season',\n",
       " 'graaaaeeeeet',\n",
       " 'stupid',\n",
       " 'wondering worth watch seasons betcha',\n",
       " 'need season',\n",
       " 'mind blowing movie',\n",
       " 'berlin piece art',\n",
       " 'good show not understand bad reviews',\n",
       " 'one best heist series history',\n",
       " 'spell perfection money heist',\n",
       " 'south american telenovela action',\n",
       " 'not like much sezons loved sezon',\n",
       " 'lost interest rd season',\n",
       " 'far best series made far',\n",
       " 'season disaster',\n",
       " 'surprisingly good',\n",
       " 'series totally gorgeous one best series',\n",
       " 'great series',\n",
       " 'great heist series',\n",
       " 'insanely amazing',\n",
       " 'tokyo overrated pathetic acting',\n",
       " 'not get hype around series',\n",
       " 'slow',\n",
       " 'please stop season',\n",
       " 'gripping',\n",
       " 'great',\n",
       " 'good',\n",
       " 'may dumb dumber',\n",
       " 'thrilling cheesy',\n",
       " 'interesting premise descends flawed plot acting',\n",
       " 'bella ciao',\n",
       " 'perfect',\n",
       " 'love hate relationship',\n",
       " 'jealousy says rated',\n",
       " 'vastly overrated',\n",
       " 'hooked',\n",
       " 'admirable robbing bank',\n",
       " 'disappointing watchable',\n",
       " 'brilliant master piece',\n",
       " 'huge potential wasted',\n",
       " 'flawed',\n",
       " 'good till end',\n",
       " 'either win lose',\n",
       " 'masterpiece series',\n",
       " 'world wide phenom nope',\n",
       " 'definetly overhyped not bad',\n",
       " 'either love hate',\n",
       " 'not take life seriously great show',\n",
       " 'overblown seasons',\n",
       " 'enough already',\n",
       " 'family tradition',\n",
       " 'best',\n",
       " 'good',\n",
       " 'meh',\n",
       " 'awesome',\n",
       " 'know',\n",
       " 'still good',\n",
       " 'not watch season',\n",
       " 'give less star',\n",
       " 'starts well',\n",
       " 'masterpiece',\n",
       " 'glad ended train wreck',\n",
       " 'perfect crime',\n",
       " 'great idea show plotholes dragged',\n",
       " 'like rd th season',\n",
       " 'season amazing',\n",
       " 'happens character study show runs character study',\n",
       " 'stop upto royal mint story bring bank spain unnecessary',\n",
       " 'best series netflix',\n",
       " 'brilliant show',\n",
       " 'best way end',\n",
       " 'season one star',\n",
       " '',\n",
       " 'diffrence seasons quality',\n",
       " 'spanish yawn',\n",
       " 'going',\n",
       " 'heist time',\n",
       " 'worst drag ever',\n",
       " 'hopelessly boring',\n",
       " 'stupid unlikable characters',\n",
       " 'mediocre could better',\n",
       " 'watching show',\n",
       " 'disney made heist movie',\n",
       " 'bet reviews comes less expectation crowd watched friends life act like real life',\n",
       " 'rating come pls',\n",
       " 'lengthen',\n",
       " 'materpiece',\n",
       " 'moon',\n",
       " 'show guys amon',\n",
       " 'rating first seasons',\n",
       " 'nice series stretched far',\n",
       " 'omg shocked masterpiece',\n",
       " 'everything good season came',\n",
       " 'greed kills show',\n",
       " 'brilliant',\n",
       " 'kindergarten class try rob bank',\n",
       " 'garbage',\n",
       " 'flawless art',\n",
       " 'utter twaddle',\n",
       " 'series great sub titles',\n",
       " 'entertains yes many nonsenses',\n",
       " 'award overhyped show horrible category',\n",
       " 'loved',\n",
       " 'unrealistic many levels',\n",
       " 'disjointed melodrama masquerading caper',\n",
       " 'predictable predictable',\n",
       " 'pure trash season',\n",
       " 'cheesy unoriginal',\n",
       " 'one dumbest shows ive ever seen',\n",
       " 'came chrasing season two',\n",
       " 'recommend series',\n",
       " 'much hype',\n",
       " 'tellin right need watch',\n",
       " 'not perfect',\n",
       " 'changing values',\n",
       " 'keystone cops',\n",
       " 'one time favourite shows',\n",
       " 'asombroso',\n",
       " 'good story weak execution',\n",
       " 'excepcional',\n",
       " 'okay',\n",
       " 'need explain',\n",
       " 'completely stupid word',\n",
       " 'good series money heist',\n",
       " 'best spanish series ever watch',\n",
       " 'ciao bella season',\n",
       " 'bloody hype come',\n",
       " 'could much better',\n",
       " 'best thrilling show seen seen many',\n",
       " 'must watch',\n",
       " 'give try',\n",
       " '',\n",
       " 'beyond expectations',\n",
       " 'waiting season',\n",
       " 'trash',\n",
       " 'good lacks luster',\n",
       " 'good seasons',\n",
       " 'interesting flawed frustrating',\n",
       " 'wow',\n",
       " 'kept end',\n",
       " 'world dumbest robbery series ever',\n",
       " 'reason success series',\n",
       " 'worst',\n",
       " 'seriously overhyped series',\n",
       " 'know stop',\n",
       " 'rob',\n",
       " 'laugh cry amazing series',\n",
       " 'heist lot unnecessary side plots',\n",
       " 'good rate',\n",
       " 'watch show like right',\n",
       " 'starts strong steadily turns annoying parody',\n",
       " 'soap opera heist genre',\n",
       " 'brilliant',\n",
       " 'waste time',\n",
       " 'not worth watching really mean',\n",
       " 'classic mistake',\n",
       " 'honors best series ever watched',\n",
       " 'good acting dialogue plot got many holes',\n",
       " 'overrated',\n",
       " 'celebration mediocrity',\n",
       " 'must watch',\n",
       " 'really fast',\n",
       " 'star review',\n",
       " 'great foreign show',\n",
       " 'meh',\n",
       " 'standards differ',\n",
       " 'potential',\n",
       " 'show ended like soprano',\n",
       " 'season one two rest',\n",
       " 'bad series reality lovers',\n",
       " 'entertaining',\n",
       " 'show corny entertaining',\n",
       " 'show breathtakingly bad',\n",
       " 'season best far',\n",
       " 'absolutely amateur show',\n",
       " 'amateur hiest show',\n",
       " 'highly overrated',\n",
       " 'power team work',\n",
       " 'kill guys already',\n",
       " 'love',\n",
       " 'entertaining utterly stupid',\n",
       " 'stop man stop',\n",
       " 'without doubt top time',\n",
       " 'una las mejores series que visto',\n",
       " 'please love god stop continuing finished stories',\n",
       " 'masterpiece',\n",
       " 'knew would sooo good',\n",
       " 'episodes stretched seasons',\n",
       " 'overacted title disappointing story',\n",
       " 'show joke',\n",
       " 'alucinante master piece',\n",
       " 'loose control',\n",
       " 'wow',\n",
       " 'spain copying hollywood much lately',\n",
       " 'one faves',\n",
       " 'usually hate heist movies',\n",
       " 'one best shows ever seen',\n",
       " 'one best thrillers',\n",
       " 'please not watch seasons',\n",
       " 'unsettling',\n",
       " 'best thriller web series',\n",
       " 'season masterpiece',\n",
       " 'overhyped',\n",
       " 'outstanding',\n",
       " 'love',\n",
       " 'one best shows time',\n",
       " 'amazing series',\n",
       " 'amazing series',\n",
       " 'perfect waste',\n",
       " 'bigger heist bigger mistakes',\n",
       " 'amazing series watch',\n",
       " 'mind blowing rd th season come',\n",
       " 'professor berlin means casa papel',\n",
       " 'series children',\n",
       " 'must see pure genius',\n",
       " 'eneded season',\n",
       " 'fascinating thought',\n",
       " 'overrated overdramatic poor cast',\n",
       " 'please favor watch spanish',\n",
       " 'must watch series',\n",
       " 'good got lot melodramatic wastage overrated',\n",
       " 'super',\n",
       " 'amazing show hard get',\n",
       " 'amazing show',\n",
       " 'overrated',\n",
       " 'okay',\n",
       " 'awesome show',\n",
       " 'want ruin good series netflix',\n",
       " 'one best show ever seen',\n",
       " 'money heist',\n",
       " 'fabulous',\n",
       " 'everyone amazing casting done nicely especially professor good personality acting',\n",
       " 'overrated',\n",
       " 'worst',\n",
       " 'amazing watch',\n",
       " 'could done better could kept seasons',\n",
       " 'mediocre',\n",
       " 'suspenseful',\n",
       " 'roller coaster emotions',\n",
       " '',\n",
       " 'overrated piece junk',\n",
       " 'brilliant series',\n",
       " 'logic heist',\n",
       " 'good series',\n",
       " 'amazing',\n",
       " 'not bad',\n",
       " 'unforgettable series',\n",
       " 'que bueno acabo glad ended',\n",
       " 'cringe cringe cringe',\n",
       " 'highly unpredictable',\n",
       " 'painfully hard watch totally overrated',\n",
       " 'mind blowing',\n",
       " 'starts well ends horribly',\n",
       " 'one best shows seen life',\n",
       " 'netflix milks series stupidly',\n",
       " 'little paper house burned',\n",
       " 'overhyped piece',\n",
       " 'ummmmm',\n",
       " 'ummmmmmm continued',\n",
       " 'ahh overhyped',\n",
       " 'really like',\n",
       " 'brilliant masterpiece',\n",
       " 'worst season got',\n",
       " 'season bad idea',\n",
       " 'think likelihood important not waste time one',\n",
       " 'average best',\n",
       " 'entertaining gripping',\n",
       " 'phenomenal',\n",
       " 'stars seasons',\n",
       " 'awful last seasons',\n",
       " 'best show seen long time',\n",
       " 'melodrama heist plots',\n",
       " 'riveting',\n",
       " 'really money heist',\n",
       " 'favor stop season',\n",
       " 'perfectly balanced',\n",
       " 'season',\n",
       " 'truly overrated',\n",
       " 'twist turns',\n",
       " ...]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_review_title = preprocess_text(data['Review title'].values)\n",
    "preprocessed_review_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e07734",
   "metadata": {},
   "source": [
    "### Preprocessing: Review body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a5ea5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some special characters, stop words, brackets in Review body texts.\n",
    "#Lets preprocess it\n",
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "# https://gist.github.com/sebleier/554280\n",
    "# we are removing the words from the stop words list: 'no', 'nor', 'not' as they contribute \n",
    "# in Negative reviews\n",
    "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\",'found','review','helpful','sign','vote','permlink']\n",
    "\n",
    "# tqdm is a library in Python which is used for creating Progress Meters or Progress Bars. \n",
    "# tqdm got its name from the Arabic name taqaddum which means 'progress'.\n",
    "from tqdm import tqdm\n",
    "def preprocess_text(text_data):\n",
    "    preprocessed_text = []\n",
    "    # tqdm is for printing the status bar\n",
    "    for sentance in tqdm(text_data):\n",
    "        sent = decontracted(sentance)\n",
    "        sent = sent.replace('\\\\r', ' ')\n",
    "        sent = sent.replace('\\\\n', ' ')\n",
    "        sent = sent.replace('\\\\\"', ' ')\n",
    "        sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "        sent = re.sub(r\"<.*>\",\"\",sent, flags=re.MULTILINE)\n",
    "        sent = re.sub(r\"[\\n\\t\\-\\\\\\/]\",\" \",sent, flags=re.MULTILINE)\n",
    "        sent = re.sub(r\"\\b_([a-zA-z]+)_\\b\",r\"\\1\",sent)  # to replace _word_ to word\n",
    "        sent = re.sub(r\"\\b_([a-zA-z]+)\\b\",r\"\\1\",sent)   # to replace_word to word\n",
    "        sent = re.sub(r\"\\b([a-zA-z]+)_\\b\",r\"\\1\",sent)   # to replace word_ to word\n",
    "        sent = re.sub(r'\\b\\w{1,2}\\b',\" \",sent) #remove words <2\n",
    "        sent = re.sub(r\"\\b\\w{15,}\\b\",\" \",sent) #remove words >15\n",
    "        sent = re.sub(r'\\d',\"\",sent, flags=re.MULTILINE)\n",
    "        # https://gist.github.com/sebleier/554280\n",
    "        sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n",
    "        preprocessed_text.append(sent.lower().strip())\n",
    "    return preprocessed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "83df4d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I expected far better than this. This Tv series is far too long, too much \"fat\", useless moments and characters, the playing is awful, unbelievable at the most, and this TV show is not brutal, not bloody enough for this kind of subject, of scheme. The producers intended to involve the largest audiences possible. Home audiences, the whole family. This brought cheesy situations, for sissies. NOT FOR ME. But I admit that there are good story lines, in the basic scheme, I mean, and interesting situations. But the whole is jeopardized by this non sense destined, I repeat, for the largest audiences possible.\n",
      "                \n",
      "                    210 out of 542 found this helpful.\n",
      "                        \n",
      "                            Was this review helpful?  Sign in to vote.\n",
      "                        \n",
      "                        \n",
      "                    Permalink\n",
      "I gave season one and two of this show a much deserved 10/10 but I'm at a loss to understand why they made season 3?Season 3 is the antithesis of the former seasons. It ruins the mostly light hearted vibe that had been established, replacing it with something much darker. Worse still, its simply, in many respects, a blatant re-hash of what went before.Like Star Wars, this feels like making something for the sake of making it. I'm happy to have had the first two seasons and will be leaving it there.5/10 for season 3.\n",
      "                \n",
      "                    70 out of 91 found this helpful.\n",
      "                        \n",
      "                            Was this review helpful?  Sign in to vote.\n",
      "                        \n",
      "                        \n",
      "                    Permalink\n",
      "I tuned into this hoping for a decent and clever bank heist tale. In some parts I got this.It feels like they first of all gave the script to some intelligent writers who conceived of these brilliant schemes to get into the bank, deal with the police and plan their escape. The kinds of antics that make the audience think 'wow, these guys are pretty smart'. The stakes are big, the planning is meticulous and the huge rewards worthy of the effort.When that was done, they then gave the script to a bunch of horny, adolescent teenagers who must have previously written 80's Australian soap operas. It is terrible. From establishing the actual job, it descends into a series of foolish characters who literally cannot keep their hands off each other and cannot keep their head in the game. They're shagging in vaults and toilets and offices. They're shagging when they are casing the joint, when they are doing the job and even with the lead investigator.From a job offering untold riches and the commitment of 5 weeks and the time for the heist, nobody can stay focused on what they are doing. They utter these silly speeches about how committed they are to the job and will kill anyone stopping it from happening but then in the next scene are willing to sacrifice it all for their sexual urges or the chance to be nice to hostages.So at the beginning, you are on the side of the criminals because their plan is so audacious but as it progresses you become more disinterested in their overly dramatic histrionics and complete lack of professionalism. There was one laughable scene where they were having another internal dispute and literally no one was supervising the hostages.Now, I'm sure people will say that you need to develop characters and yes you do but when those characters are acting so immaturely, so petulantly and so lacking in thought or consideration of the bigger picture, you just kinda get bored with them. You start looking for the most stupid and heist wrecking thing that they could do and laughing when they do it. I didn't tune in for a romantic drama but a bank heist and when the characters are behaving like silly teenagers, the tethers to your interest just withers away.You just want someone to shake these people and say 'we're talking about millions of Euro's each here. Is it possible you could remain on target for literally just the time of the heist (2/3 days) and then play out your love chasing routines when the job is done?' But even the guy at the very top is distracted by skirt so they all descend into silly unengaging children playing robbers.So overall, it was a fascinating concept that was ruined, and I do mean ruined, by characters acting so idiotically that they become laughable and unlikeable. They go from the no nonsense crew in Heat to a bunch of sexed up moronic clowns.\n"
     ]
    }
   ],
   "source": [
    "print(data['Review_body'][2])\n",
    "print(data['Review_body'][12])\n",
    "print(data['Review_body'][67])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "89965b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3466/3466 [00:01<00:00, 2555.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected far better series far long much fat useless moments characters playing awful unbelievable show not brutal not bloody enough kind subject scheme producers intended involve largest audiences possible home audiences whole family brought cheesy situations sissies not admit good story lines basic scheme mean interesting situations whole jeopardized non sense destined repeat largest audiences possible permalink\n",
      "gave season one two show much deserved loss understand made season season antithesis former seasons ruins mostly light hearted vibe established replacing something much darker worse still simply many respects blatant hash went like star wars feels like making something sake making happy first two seasons leaving season permalink\n",
      "tuned hoping decent clever bank heist tale parts got feels like first gave script intelligent writers conceived brilliant schemes get bank deal police plan escape kinds antics make audience think wow guys pretty smart stakes big planning meticulous huge rewards worthy effort done gave script bunch horny adolescent teenagers must previously written australian soap operas terrible establishing actual job descends series foolish characters literally cannot keep hands cannot keep head game shagging vaults toilets offices shagging casing joint job even lead investigator job offering untold riches commitment weeks time heist nobody stay focused utter silly speeches committed job kill anyone stopping happening next scene willing sacrifice sexual urges chance nice hostages beginning side criminals plan audacious progresses become disinterested overly dramatic histrionics complete lack one laughable scene another internal dispute literally one supervising hostages sure people say need develop characters yes characters acting immaturely petulantly lacking thought consideration bigger picture kinda get bored start looking stupid heist wrecking thing could laughing not tune romantic drama bank heist characters behaving like silly teenagers tethers interest withers away want someone shake people say talking millions euro possible could remain target literally time heist days play love chasing routines job done even guy top distracted skirt descend silly unengaging children playing robbers overall fascinating concept ruined mean ruined characters acting idiotically become laughable unlikeable nonsense crew heat bunch sexed moronic clowns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_review_body = preprocess_text(data['Review_body'].values)\n",
    "print(preprocessed_review_body[2])\n",
    "print(preprocessed_review_body[12])\n",
    "print(preprocessed_review_body[67])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8f7be1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=[8,9,10]\n",
    "neut=[4,5,6,7]\n",
    "neg=[1,2,3]\n",
    "preprocessed_ratings=[]\n",
    "for i in data['Review Rating']:\n",
    "    if i in neg:\n",
    "        preprocessed_ratings.append(0)       # Assigning 0 for negative reviews\n",
    "    elif i in neut:\n",
    "        preprocessed_ratings.append(1)       # Assigning 1 for neutral reviews\n",
    "    elif i in pos:\n",
    "        preprocessed_ratings.append(2)       # Assigning 2 for positive reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f0be5516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3466\n",
      "3466\n",
      "3466\n"
     ]
    }
   ],
   "source": [
    "print(len(preprocessed_review_title))\n",
    "print(len(preprocessed_review_body))\n",
    "print(len(preprocessed_ratings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d680dadb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>theft heft</td>\n",
       "      <td>one many great series netflix depicts group pe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>awesome spanish series plenty thrills action t...</td>\n",
       "      <td>creator alex pina last one results splendid se...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mess</td>\n",
       "      <td>expected far better series far long much fat u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clever intriguing initially gets dumber dumber...</td>\n",
       "      <td>band robbers lead man known simply nothe profe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>watch spanish</td>\n",
       "      <td>friends talking casa papel really not get call...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3461</th>\n",
       "      <td>idea good messed characters many points unconv...</td>\n",
       "      <td>idea good messed characters many points unconv...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3462</th>\n",
       "      <td>shame</td>\n",
       "      <td>creators actors supporting child killers zioni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>movie sweet die another way saying movie super...</td>\n",
       "      <td>wish morethan stars rate movie definitely time...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3464</th>\n",
       "      <td>awesome</td>\n",
       "      <td>one best shows watched far excellent work perm...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3465</th>\n",
       "      <td>difficult task writers</td>\n",
       "      <td>ending part series uncertain story would turn ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3466 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           review_title  \\\n",
       "0                                            theft heft   \n",
       "1     awesome spanish series plenty thrills action t...   \n",
       "2                                                  mess   \n",
       "3     clever intriguing initially gets dumber dumber...   \n",
       "4                                         watch spanish   \n",
       "...                                                 ...   \n",
       "3461  idea good messed characters many points unconv...   \n",
       "3462                                              shame   \n",
       "3463  movie sweet die another way saying movie super...   \n",
       "3464                                            awesome   \n",
       "3465                             difficult task writers   \n",
       "\n",
       "                                            review_text  class  \n",
       "0     one many great series netflix depicts group pe...      2  \n",
       "1     creator alex pina last one results splendid se...      2  \n",
       "2     expected far better series far long much fat u...      0  \n",
       "3     band robbers lead man known simply nothe profe...      1  \n",
       "4     friends talking casa papel really not get call...      2  \n",
       "...                                                 ...    ...  \n",
       "3461  idea good messed characters many points unconv...      1  \n",
       "3462  creators actors supporting child killers zioni...      0  \n",
       "3463  wish morethan stars rate movie definitely time...      2  \n",
       "3464  one best shows watched far excellent work perm...      2  \n",
       "3465  ending part series uncertain story would turn ...      2  \n",
       "\n",
       "[3466 rows x 3 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data= pd.DataFrame()\n",
    "final_data['review_title']= preprocessed_review_title\n",
    "final_data['review_text']= preprocessed_review_body\n",
    "final_data['class']= preprocessed_ratings\n",
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5d0f1719",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('preprocessed_data.csv')   # storing it to csv file for further use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501fc879",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "500865e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.read_csv('preprocessed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cca8d4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>theft heft</td>\n",
       "      <td>one many great series netflix depicts group pe...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>awesome spanish series plenty thrills action t...</td>\n",
       "      <td>creator alex pina last one results splendid se...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>mess</td>\n",
       "      <td>expected far better series far long much fat u...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>clever intriguing initially gets dumber dumber...</td>\n",
       "      <td>band robbers lead man known simply nothe profe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>watch spanish</td>\n",
       "      <td>friends talking casa papel really not get call...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                       review_title  \\\n",
       "0           0                                         theft heft   \n",
       "1           1  awesome spanish series plenty thrills action t...   \n",
       "2           2                                               mess   \n",
       "3           3  clever intriguing initially gets dumber dumber...   \n",
       "4           4                                      watch spanish   \n",
       "\n",
       "                                         review_text  class  \n",
       "0  one many great series netflix depicts group pe...      2  \n",
       "1  creator alex pina last one results splendid se...      2  \n",
       "2  expected far better series far long much fat u...      0  \n",
       "3  band robbers lead man known simply nothe profe...      1  \n",
       "4  friends talking casa papel really not get call...      2  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "61a88cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing nan values with name of the series\n",
    "data['review_title'] = data['review_title'].replace(np.nan, 'money heist') \n",
    "data['review_text'] = data['review_text'].replace(np.nan, 'money heist') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c73dcc",
   "metadata": {},
   "source": [
    "# Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "41a35b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orinial data point shape:  (3466, 2)\n",
      "Orinial class label shape:  (3466,)\n",
      "Training data shape:  (2772, 2)\n",
      "Training class label shape:  (2772,)\n",
      "Test data shape:  (694, 2)\n",
      "Test class label shape:  (694,)\n"
     ]
    }
   ],
   "source": [
    "y = data['class'].values\n",
    "X = data.drop(columns=['class','Unnamed: 0'], axis=1)\n",
    "\n",
    "print('Orinial data point shape: ', X.shape)\n",
    "print('Orinial class label shape: ', y.shape)\n",
    "\n",
    "#Splitting the data into train and test.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y)\n",
    "\n",
    "print('Training data shape: ',X_train.shape)\n",
    "print('Training class label shape: ',y_train.shape)\n",
    "print('Test data shape: ',X_test.shape)\n",
    "print('Test class label shape: ',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2a0db",
   "metadata": {},
   "source": [
    "# Make Data Model Ready: encoding review_title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f64d10",
   "metadata": {},
   "source": [
    "# TF-IDF vectorization of review_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "31668c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2772, 5000) (2772,)\n",
      "(694, 5000) (694,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "vectorizertfidf = TfidfVectorizer(min_df=1,ngram_range=(1,4), max_features=5000)    # limiting the no. of features =5000\n",
    "vectorizertfidf.fit(X_train['review_title'].values) # fit has to happen only on train data\n",
    "\n",
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "X_train_title_tfidf = vectorizertfidf.transform(X_train['review_title'].values)\n",
    "X_test_title_tfidf = vectorizertfidf.transform(X_test['review_title'].values)\n",
    "\n",
    "print(X_train_title_tfidf.shape, y_train.shape)\n",
    "print(X_test_title_tfidf.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be633f4",
   "metadata": {},
   "source": [
    "# TFIDF_W2V vectorization of review_title\n",
    "\n",
    "## Text Vectorization is the process of converting text into numerical representation. \n",
    "### Bag of Words just creates a set of vectors containing the count of word occurrences in the document (reviews), while the TF-IDF model contains information on the more important words and the less important ones as well.\n",
    "#### TF-IDF stands for term frequency-inverse document frequency and it is a measure, used in the fields of information retrieval (IR) and machine learning\n",
    "\n",
    "### Stemming is used to normalize words into its base form or root form. For example, celebrates, celebrated and celebrating, all these words are originated with a single root word \"celebrate.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "394e1b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are converting a dictionary with word as a key, and the idf as a value\n",
    "title_dictionary = dict(zip(vectorizertfidf.get_feature_names(), list(vectorizertfidf.idf_)))\n",
    "tfidf_title_words = set(vectorizertfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7994c018",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'glove_vectors'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23456/410644407.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#to load glove vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'glove_vectors'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mglove_words\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'glove_vectors'"
     ]
    }
   ],
   "source": [
    "#to load glove vectors \n",
    "with open('glove_vectors', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "    glove_words =  set(model.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93df6f4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glove_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23456/565154761.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtfidf_w2v_vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mX_train_title_tfidf_w2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidfw2v\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mglove_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtfidf_title_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtitle_dictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mX_test_title_tfidf_w2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidfw2v\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mglove_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtfidf_title_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtitle_dictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glove_words' is not defined"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "def tfidfw2v(data,glove_words,tfidf_words,model,dictionary):\n",
    "    \"\"\"This function transforms sentences into tfidf weighted vectors and returns dataset of such vectors\"\"\"\n",
    "    tfidf_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "    for sentence in tqdm(data['review_title'].values): # for each review/sentence\n",
    "        vector = np.zeros(300) # as word vectors are of zero length\n",
    "        tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n",
    "        for word in sentence.split(): # for each word in a review/sentence\n",
    "            if (word in glove_words) and (word in tfidf_words):\n",
    "                vec = model[word] # getting the vector for each word\n",
    "                # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n",
    "                tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n",
    "                vector += (vec * tf_idf) # calculating tfidf weighted w2v\n",
    "                tf_idf_weight += tf_idf\n",
    "        if tf_idf_weight != 0:\n",
    "            vector /= tf_idf_weight\n",
    "        tfidf_w2v_vectors.append(vector)\n",
    "    return tfidf_w2v_vectors\n",
    "\n",
    "X_train_title_tfidf_w2v = tfidfw2v(X_train,glove_words,tfidf_title_words,model,title_dictionary)\n",
    "X_test_title_tfidf_w2v = tfidfw2v(X_test,glove_words,tfidf_title_words,model,title_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "112aecec",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_title_tfidf_w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23456/924771474.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_title_tfidf_w2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_title_tfidf_w2v\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_title_tfidf_w2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_title_tfidf_w2v\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_title_tfidf_w2v' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(len(X_train_title_tfidf_w2v))\n",
    "print(len(X_train_title_tfidf_w2v[0]))\n",
    "print(len(X_test_title_tfidf_w2v))\n",
    "print(len(X_test_title_tfidf_w2v[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4602f8f",
   "metadata": {},
   "source": [
    "# Make Data Model Ready: encoding review_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eca6919",
   "metadata": {},
   "source": [
    "# TF-IDF vectorization of review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7b89ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2772, 5000) (2772,)\n",
      "(694, 5000) (694,)\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "vectorizertfidf_t = TfidfVectorizer(min_df=1,ngram_range=(1,4), max_features=5000)    # limiting the no. of features =5000\n",
    "vectorizertfidf_t.fit(X_train['review_text'].values) # fit has to happen only on train data\n",
    "\n",
    "# we use the fitted CountVectorizer to convert the text to vector\n",
    "X_train_text_tfidf = vectorizertfidf_t.transform(X_train['review_text'].values)\n",
    "X_test_text_tfidf = vectorizertfidf_t.transform(X_test['review_text'].values)\n",
    "\n",
    "print(X_train_text_tfidf.shape, y_train.shape)\n",
    "print(X_test_text_tfidf.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d56a2",
   "metadata": {},
   "source": [
    "# TFIDF_W2V vectorization of review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a32efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are converting a dictionary with word as a key, and the idf as a value\n",
    "text_dictionary = dict(zip(vectorizertfidf_t.get_feature_names(), list(vectorizertfidf_t.idf_)))\n",
    "tfidf_text_words = set(vectorizertfidf_t.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6962064c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glove_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23456/157642418.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtfidf_w2v_vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m \u001b[0mX_train_text_tfidf_w2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidfw2v\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mglove_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtfidf_text_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext_dictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[0mX_test_text_tfidf_w2v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidfw2v\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mglove_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtfidf_text_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtext_dictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glove_words' is not defined"
     ]
    }
   ],
   "source": [
    "# average Word2Vec\n",
    "# compute average word2vec for each review.\n",
    "def tfidfw2v(data,glove_words,tfidf_words,model,dictionary):\n",
    "    \"\"\"This function transforms sentences into tfidf weighted vectors and returns dataset of such vectors\"\"\"\n",
    "    tfidf_w2v_vectors = []; # the avg-w2v for each sentence/review is stored in this list\n",
    "    for sentence in tqdm(data['review_text'].values): # for each review/sentence\n",
    "        vector = np.zeros(300) # as word vectors are of zero length\n",
    "        tf_idf_weight =0; # num of words with a valid vector in the sentence/review\n",
    "        for word in sentence.split(): # for each word in a review/sentence\n",
    "            if (word in glove_words) and (word in tfidf_words):\n",
    "                vec = model[word] # getting the vector for each word\n",
    "                # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)/len(sentence.split())))\n",
    "                tf_idf = dictionary[word]*(sentence.count(word)/len(sentence.split())) # getting the tfidf value for each word\n",
    "                vector += (vec * tf_idf) # calculating tfidf weighted w2v\n",
    "                tf_idf_weight += tf_idf\n",
    "        if tf_idf_weight != 0:\n",
    "            vector /= tf_idf_weight\n",
    "        tfidf_w2v_vectors.append(vector)\n",
    "    return tfidf_w2v_vectors\n",
    "\n",
    "X_train_text_tfidf_w2v = tfidfw2v(X_train,glove_words,tfidf_text_words,model,text_dictionary)\n",
    "X_test_text_tfidf_w2v = tfidfw2v(X_test,glove_words,tfidf_text_words,model,text_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d232a63c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_text_tfidf_w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23456/2169319153.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_text_tfidf_w2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_text_tfidf_w2v\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_text_tfidf_w2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_text_tfidf_w2v\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_text_tfidf_w2v' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(X_train_text_tfidf_w2v))\n",
    "print(len(X_train_text_tfidf_w2v[0]))\n",
    "print(len(X_test_text_tfidf_w2v))\n",
    "print(len(X_test_text_tfidf_w2v[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ebdcbf",
   "metadata": {},
   "source": [
    "# Sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "034a61e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\DS_USER/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\DS_USER\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23456/3461451725.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentiment_score_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mX_train_sentiment_scores\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msentiment_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mX_test_sentiment_scores\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0msentiment_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23456/3461451725.py\u001b[0m in \u001b[0;36msentiment_score\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msentiment_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;34m\"\"\"This function returns vector containing lists of sentiment scores for each sentence or essay\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0msentiment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0msentiment_score_vector\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\sentiment\\vader.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lexicon_file)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mlexicon_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m     ):\n\u001b[1;32m--> 340\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlexicon_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_lex_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVaderConstants\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    874\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 876\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    877\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    878\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mvader_lexicon\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('vader_lexicon')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93msentiment/vader_lexicon.zip/vader_lexicon/vader_lexicon.txt\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\DS_USER/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\DS_USER\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "def sentiment_score(data):\n",
    "    \"\"\"This function returns vector containing lists of sentiment scores for each sentence or essay\"\"\"\n",
    "    sentiment = SentimentIntensityAnalyzer()\n",
    "    sentiment_score_vector=[]\n",
    "    for i in tqdm(data['review_text'].values):\n",
    "        ss= sentiment.polarity_scores(i)\n",
    "        sentiment_score_vector.append(list(ss.values()))\n",
    "    return np.array(sentiment_score_vector)\n",
    "        \n",
    "X_train_sentiment_scores= sentiment_score(X_train)\n",
    "X_test_sentiment_scores= sentiment_score(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cc2a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "23121aef",
   "metadata": {},
   "source": [
    "## Concatinating all the features for Set1  (Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80eca82c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_sentiment_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23456/3384054440.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_tr1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_title_tfidf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train_text_tfidf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train_sentiment_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_te1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_title_tfidf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test_text_tfidf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test_sentiment_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Final Data matrix\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_sentiment_scores' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_tr1 = hstack((X_train_title_tfidf,X_train_text_tfidf,X_train_sentiment_scores)).tocsr()\n",
    "X_te1 = hstack((X_test_title_tfidf,X_test_text_tfidf,X_test_sentiment_scores)).tocsr()\n",
    "\n",
    "print(\"Final Data matrix\")\n",
    "print(X_tr1.shape, y_train.shape)\n",
    "print(X_te1.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2084efe",
   "metadata": {},
   "source": [
    "## Concatinating all the features for Set2 (Tfidf_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e4b66c6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_title_tfidf_w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23456/1907725585.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train_title_tfidf_w2v\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_title_tfidf_w2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_train_text_tfidf_w2v\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_text_tfidf_w2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mX_test_title_tfidf_w2v\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_title_tfidf_w2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_test_text_tfidf_w2v\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_text_tfidf_w2v\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_title_tfidf_w2v' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_title_tfidf_w2v=np.array(X_train_title_tfidf_w2v)\n",
    "X_train_text_tfidf_w2v=np.array(X_train_text_tfidf_w2v)\n",
    "X_test_title_tfidf_w2v=np.array(X_test_title_tfidf_w2v)\n",
    "X_test_text_tfidf_w2v=np.array(X_test_text_tfidf_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2045206a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_title_tfidf_w2v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23456/1355665845.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mhstack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_tr2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_title_tfidf_w2v\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train_text_tfidf_w2v\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_train_sentiment_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mX_te2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_title_tfidf_w2v\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test_text_tfidf_w2v\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test_sentiment_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Final Data matrix\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_title_tfidf_w2v' is not defined"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "X_tr2 = np.hstack((X_train_title_tfidf_w2v,X_train_text_tfidf_w2v,X_train_sentiment_scores))\n",
    "X_te2 = np.hstack((X_test_title_tfidf_w2v,X_test_text_tfidf_w2v,X_test_sentiment_scores))\n",
    "\n",
    "print(\"Final Data matrix\")\n",
    "print(X_tr2.shape, y_train.shape)\n",
    "print(X_te2.shape, y_test.shape)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dc1ada48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(test_y, predict_y):\n",
    "    C = confusion_matrix(test_y, predict_y)\n",
    "    print(\"Number of misclassified points \",(len(test_y)-np.trace(C))/len(test_y)*100)\n",
    "    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n",
    "    \n",
    "    A =(((C.T)/(C.sum(axis=1))).T)\n",
    "    \n",
    "    B =(C/C.sum(axis=0))\n",
    "    \n",
    "    labels = [0,1,2]\n",
    "    cmap=sns.light_palette(\"green\")\n",
    "    # representing A in heatmap format\n",
    "    print(\"-\"*50, \"Confusion matrix\", \"-\"*50)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"-\"*50, \"Precision matrix\", \"-\"*50)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.show()\n",
    "    print(\"Sum of columns in precision matrix\",B.sum(axis=0))\n",
    "    \n",
    "    # representing B in heatmap format\n",
    "    print(\"-\"*50, \"Recall matrix\"    , \"-\"*50)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.show()\n",
    "    print(\"Sum of rows in precision matrix\",A.sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da992f9",
   "metadata": {},
   "source": [
    "# Model: SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06829924",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning for set(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b532ab8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tr1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23456/706733545.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msvc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'f1_micro'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_tr1' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "parameters={'kernel':['linear', 'rbf', 'poly'], \"C\":[0.00001,0.0005, 0.0001,0.005,0.001,0.05,0.01,0.1,0.5,1,5,10,50,100]}\n",
    "\n",
    "svc = SVC()\n",
    "clf = GridSearchCV(svc, parameters, cv=3, scoring='f1_micro',return_train_score=True)\n",
    "clf.fit(X_tr1, y_train)\n",
    "\n",
    "results = pd.DataFrame.from_dict(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f4879eb6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23456/962467392.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e00680f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_params_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23456/3788034823.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_params_'"
     ]
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d983743c",
   "metadata": {},
   "source": [
    "## Training SVC with best hyperparameter on set1(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6e5b774c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tr1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23456/1162575760.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msvc_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0msvc_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0msig_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCalibratedClassifierCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvc_classifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0msig_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_tr1' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "svc_classifier = SVC(kernel='linear', C=0.5, probability=True)\n",
    "svc_classifier.fit(X_tr1,y_train)\n",
    "sig_clf = CalibratedClassifierCV(svc_classifier, method=\"sigmoid\")\n",
    "sig_clf.fit(X_tr1,y_train)\n",
    "\n",
    "predict_y = sig_clf.predict_proba(X_tr1)\n",
    "print ('For values of best C = ', 0.5, \"The train log loss is:\",log_loss(y_train, predict_y))\n",
    "predict_y = sig_clf.predict_proba(X_te1)\n",
    "print('For values of best alpha = ', 0.5, \"The test log loss is:\",log_loss(y_test, predict_y))\n",
    "plot_confusion_matrix(y_test, sig_clf.predict(X_te1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c52dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding False Positive data points from set1 predicted by classifier\n",
    "test_pred_class=sig_clf.predict(X_te1)     \n",
    "false_positive_set1 = X_test[(y_test == 0) & (test_pred_class == 2)] #False Positives data points from set1\n",
    "print(len(false_positive_set1))\n",
    "# Refrence:https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "# iterate through the csv file\n",
    "for sentence in false_positive_set1.review_text:\n",
    "     \n",
    "    # typecaste each val to string\n",
    "    sentence = str(sentence)\n",
    " \n",
    "    # split the value\n",
    "    tokens = sentence.split()\n",
    "     \n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "     \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    " \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    " \n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60622005",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning for set(2) tfidf_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd79f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={'kernel':['linear', 'rbf', 'poly'], \"C\":[0.00001,0.0005, 0.0001,0.005,0.001,0.05,0.01,0.1,0.5,1,5,10,50,100]}\n",
    "\n",
    "svc1 = SVC()\n",
    "clf1 = GridSearchCV(svc1, parameters, cv=3, scoring='f1_micro',return_train_score=True)\n",
    "clf1.fit(X_tr2, y_train)\n",
    "\n",
    "results1 = pd.DataFrame.from_dict(clf1.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5278b768",
   "metadata": {},
   "outputs": [],
   "source": [
    "results1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c58de",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca67d6a",
   "metadata": {},
   "source": [
    "## Training SVC with best hyperparameter on set2(tfidf_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40d7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier1 = SVC(kernel='rbf', C=1, probability=True)\n",
    "svc_classifier1.fit(X_tr2,y_train)\n",
    "sig_clf1 = CalibratedClassifierCV(svc_classifier1, method=\"sigmoid\")\n",
    "sig_clf1.fit(X_tr2,y_train)\n",
    "\n",
    "predict_y = sig_clf1.predict_proba(X_tr2)\n",
    "print ('For values of best C = ', 1, \"The train log loss is:\",log_loss(y_train, predict_y))\n",
    "predict_y = sig_clf1.predict_proba(X_te2)\n",
    "print('For values of best alpha = ', 1, \"The test log loss is:\",log_loss(y_test, predict_y))\n",
    "plot_confusion_matrix(y_test, sig_clf1.predict(X_te2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9c3fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding False Positive data points from set1 predicted by classifier\n",
    "test_pred_class=sig_clf1.predict(X_te2)     \n",
    "false_positive_set2 = X_test[(y_test == 0) & (test_pred_class == 2)] #False Positives data points from set1\n",
    "print(len(false_positive_set2))\n",
    "# Refrence:https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "# iterate through the csv file\n",
    "for sentence in false_positive_set2.review_text:\n",
    "     \n",
    "    # typecaste each val to string\n",
    "    sentence = str(sentence)\n",
    " \n",
    "    # split the value\n",
    "    tokens = sentence.split()\n",
    "     \n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "     \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    " \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    " \n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe90f1a",
   "metadata": {},
   "source": [
    "# Model: XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454ab920",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning for set(1) tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c9a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "parameters={'n_estimators':[10,50,75,100,150,200,500]}\n",
    "xgb = XGBClassifier(verbosity = 0)\n",
    "clf_xgb = GridSearchCV(xgb, parameters, cv=3, scoring='f1_micro',return_train_score=True)\n",
    "clf_xgb.fit(X_tr1, y_train)\n",
    "results_xgb = pd.DataFrame.from_dict(clf_xgb.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598c6326",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627f7884",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19eeb6fb",
   "metadata": {},
   "source": [
    "## Training XGBoost with best hyperparameter on set1(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c300ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(n_estimators=100,max_depth=10,learning_rate=0.15,colsample_bytree=0.3,subsample=1,nthread=-1)\n",
    "xgb.fit(X_tr1,y_train,verbose=True)\n",
    "sig_clf_xgb = CalibratedClassifierCV(xgb, method=\"sigmoid\")\n",
    "sig_clf_xgb.fit(X_tr1, y_train)\n",
    "\n",
    "predict_y = sig_clf_xgb.predict_proba(X_tr1)\n",
    "print ('For values of best n_estimators = ', 100, \"The train log loss is:\",log_loss(y_train, predict_y))\n",
    "predict_y = sig_clf_xgb.predict_proba(X_te1)\n",
    "print('For values of best n_estimators = ', 100, \"The test log loss is:\",log_loss(y_test, predict_y))\n",
    "plot_confusion_matrix(y_test, sig_clf_xgb.predict(X_te1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789209ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding False Positive data points from set1 predicted by classifier\n",
    "test_pred_class=sig_clf_xgb.predict(X_te1)     \n",
    "false_positive_set3 = X_test[(y_test == 0) & (test_pred_class == 2)] #False Positives data points from set1\n",
    "print(len(false_positive_set3))\n",
    "# Refrence:https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "# iterate through the csv file\n",
    "for sentence in false_positive_set3.review_text:\n",
    "     \n",
    "    # typecaste each val to string\n",
    "    sentence = str(sentence)\n",
    " \n",
    "    # split the value\n",
    "    tokens = sentence.split()\n",
    "     \n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "     \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    " \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    " \n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1424878",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning for set(2) tfidf_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03308f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "parameters={'n_estimators':[10,50,75,100,150,200,500]}\n",
    "xgb1 = XGBClassifier(verbosity = 0)\n",
    "clf_xgb1 = GridSearchCV(xgb1, parameters, cv=3, scoring='f1_micro',return_train_score=True)\n",
    "clf_xgb1.fit(X_tr2, y_train)\n",
    "results_xgb1 = pd.DataFrame.from_dict(clf_xgb1.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7b0215",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_xgb1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769d6c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb1.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a529d439",
   "metadata": {},
   "source": [
    "## Training XGBoost with best hyperparameter on set2(tfidf_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de80c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(n_estimators=500,max_depth=10,learning_rate=0.15,colsample_bytree=0.3,subsample=1,nthread=-1)\n",
    "xgb1.fit(X_tr2,y_train,verbose=True)\n",
    "sig_clf_xgb1 = CalibratedClassifierCV(xgb1, method=\"sigmoid\")\n",
    "sig_clf_xgb1.fit(X_tr2, y_train)\n",
    "\n",
    "predict_y = sig_clf_xgb1.predict_proba(X_tr2)\n",
    "print ('For values of best n_estimators = ', 100, \"The train log loss is:\",log_loss(y_train, predict_y))\n",
    "predict_y = sig_clf_xgb1.predict_proba(X_te2)\n",
    "print('For values of best n_estimators = ', 100, \"The test log loss is:\",log_loss(y_test, predict_y))\n",
    "plot_confusion_matrix(y_test, sig_clf_xgb1.predict(X_te2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3055ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding False Positive data points from set1 predicted by classifier\n",
    "test_pred_class=sig_clf_xgb1.predict(X_te2)     \n",
    "false_positive_set4 = X_test[(y_test == 0) & (test_pred_class == 2)] #False Positives data points from set1\n",
    "print(len(false_positive_set4))\n",
    "# Refrence:https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "comment_words = ''\n",
    "stopwords = set(STOPWORDS)\n",
    "# iterate through the csv file\n",
    "for sentence in false_positive_set4.review_text:\n",
    "     \n",
    "    # typecaste each val to string\n",
    "    sentence = str(sentence)\n",
    " \n",
    "    # split the value\n",
    "    tokens = sentence.split()\n",
    "     \n",
    "    # Converts each token into lowercase\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "     \n",
    "    comment_words += \" \".join(tokens)+\" \"\n",
    " \n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                background_color ='white',\n",
    "                stopwords = stopwords,\n",
    "                min_font_size = 10).generate(comment_words)\n",
    " \n",
    "# plot the WordCloud image                      \n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    " \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44a2498",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference: https://www.geeksforgeeks.org/creating-tables-with-prettytable-library-python/\n",
    "from prettytable import PrettyTable\n",
    "t = PrettyTable(['Vectorizer', 'Model','train log loss','test log loss'])\n",
    "t.add_row(['Tfidf', 'SVC',0.3905,0.6785])\n",
    "t.add_row(['Tfidf_w2v', 'SVC',0.6276,0.7669])\n",
    "t.add_row(['Tfidf', 'XGBoost',0.2743,0.7017])\n",
    "t.add_row(['Tfidf_w2v', 'XGBoost',0.2690,0.7183])\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9875e485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF is better than Count Vectorizers because it not only focuses on \n",
    "# the frequency of words present in the corpus but also provides the importance of the words. \n",
    "# We can then remove the words that are less important for analysis, hence making the \n",
    "# model building less complex by reducing the input dimensions.\n",
    "\n",
    "# tqdm is a library in Python which is used for creating Progress Meters or Progress Bars.\n",
    "# tqdm got its name from the Arabic name taqaddum which means 'progress'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab8e83d",
   "metadata": {},
   "source": [
    "### For any given problem, a lower log loss value means better predictions.\n",
    "\n",
    "# Conclusion:\n",
    "\n",
    "\n",
    "1. On the acquired data vectorization using tfidf and SVC with linear kernel gives less test log loss. Also number of misclassified points are also less.\n",
    "\n",
    "2. The words like 'series', 'chrctr', 'seson', 'show' etc have high importance as shown in the World Cloud.\n",
    "\n",
    "3. Even though some words in the text don't have particular meaning, they are contributing in defining the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c768cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
